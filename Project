#########################
Day 32 : 2nd Sep. 2025
#########################........



	- Java Spring App Developer Certification with DevOps .
	
		- Full-stack Developer with DevOps 
		
		
	:::
	
	DevOps Assessment :
	
		-> Create CI-CD Pipeline
		
		
	Pre-requisites ????
	
	
	

	DevOps Assessment :
	
		- Interact with all the teams and collect the detailed requirements 
		
		
		
	Phase 1 :
		-> 	Infra-Structure Provisioning and Configurations 
				- Using Terraform and Ansible 
				- Using AWS Console 
		
	
	Phase 2 :
		-> 	Automation of Application Builds & Deployments  
				- Using Jenkins CICD Pipelines
		
		
	Phase 3 :
		-> 	Monitoring of Infra-Structure
				- Using Prometheus & Grafana
				
				
	What are the Servers(VM) needed ?
	
	What are the tools to be configured in each server ?
	
	What are the stages of CICD Pipeline ?

			
Implementation Procedures/Steps :::	

	
	
	
		1. List of Servers :: 
		===========================
			Infra-Structure Management Team's Perspectives :
			
				- Create using AWS Console 
				
				- Create using Terraform / Ansible 
				
						VM ==> Install Terraform & Ansible 
				
				
			
			On DevOps Perspectives :	# Create CICD Pipeline ::			

				- Jenkins_Master 						VM1 	
					- Jenkins_SlaveNode(Build_Server)	VM2 
				
				- Kubernetes_Master						VM3
					- Kubernetes_WorkerNode1			VM4 
					- Kubernetes_WorkerNode2			VM5 
					- Kubernetes_WorkerNode3			VM6
		
		
			Production Support/Monitor Teams' Perspective 
		
				- Monitor the Kubernetes Cluster/Jenkins_Slave_Nodes using Prometheus & Grafana :				
				
					- Dedicated Monitoring Server! 	VM						# Monitor Jenkins_Master & Jenkins_Slave_Nodes
				
					- Dedicated Monitoring Services(Prometheus & Grafana)	# To Monitor Kubernetes Cluster & Pods
				
				
			Servers & it's purpose :::
			=============================
			
				- Jenkins_Master 						VM1 	# To create CICD Pipeline Jobs and schedule it to run in Slave Nodes	
				
					- Jenkins_SlaveNode(Build_Server)	VM2 	# Checkout the Source_Code for GIT
																# Compile the Source Code
																# Create Artifacts
																# Perform Unit Testing 
																# Create Application Image
																# Publish the Application Image to Container Registry
																
				
				- Kubernetes_Master						VM3		# Schedule the Pods Deployment 
																# Execute Kubernetes Monitoring Services using Prometheus & Grafana
													
					- Kubernetes_WorkerNode1			VM4 	# Execute the Pods
					- Kubernetes_WorkerNode2			VM5 	# Execute the Pods
					- Kubernetes_WorkerNode3			VM6		# Execute the Pods	
				
				
				- Monitoring Server 					VM7		# To Monitor Jenkins_Master & Jenkins_Slave_Nodes
																# Create Monitoring Dashboards
				
				
				
				
				
		2. List of Tools :: 				
				
				- Jenkins_Master 						VM1 	# git,jdk,jenkins
					- Jenkins_SlaveNode(Build_Server)	VM2 	# git,jdk,maven,docker
					
				- Kubernetes_Master						VM3		# All Kubernetes Components	
					- Kubernetes_WorkerNode1			VM4 
					- Kubernetes_WorkerNode2			VM5 
					- Kubernetes_WorkerNode3			VM6
					
				
				Manage the Credentials to access DockerHub,Jenkins_SlaveNodes,Kubernetes,Github(private repo)
				
						goto jenkins dashboard 
							-- Manage Jenkins 
								-- Credential 
									-- using dockerhub username and access token				

		Integration of Kubernetes Master Node with Jenkins Master Node.
		
			- Using Publish over ssh plugins, integrate Kubernetes_Master Node to Jenkins Master Node
			
			- And Execute kubectl create command using Publish over ssh plugins
			
			--> 1. Create a devopsadmin - User in Kubernetes_Master
			
			--> 2. Create SSH Keys to devopsadmin - User
			
			--> 3. Ensure that devopsadmin - User have access to execute kubectl commands
			
			--> 4. Use, the Kubernetes - Master Node's - Private IP Address,User_Name and Credential to config in Jenkins - Publish Over SSH
			
			--> 5. Create a Pipeline Deployment Stage using Publish Over SSH Plugins step
					- To copy the kubernetes manifest file - *.yaml file from jenkins_Slave Node to Kubernetes Master Node.
					- To execute a command : kubectl apply -f kubernetesdeploy.yaml
				
		
		Enhance the project using github webhook(Make the code changes and trigger the build) & Email Notification				
				
				


		3. CICD Pipeline Stages :::
		
			- SCM Checkout 
			
			- Application Build 
			
			- Application Image Build
			
			- Login to DockerHub 

			- Publish Application Image to DockerHub Container Registry
			
			- Deploy the Application Images to Target Environments(QA/PROD) using Kubernetes				
					
					Application Image: 
					
						Loksaieta/mywebapp:${BUILD_NUMBER}
						Loksaieta/mywebapp:${BUILD_NUMBER}
						Loksaieta/mywebapp:${BUILD_NUMBER}
						
						
						
		Source_Code 				Artifacts 			Application_Image 
		
		
			webapp_Code_v1.0	==>	webapp.war_v1.0	 ==> webapp_image:v1.0	 ==> Deployed to QA/UAT/PROD
					
				
			webapp_Code_v1.1	==>	webapp.war_v1.1	 ==> webapp_image:v1.1	 ==> Deployed to QA/UAT/PROD			
				
				
				
		Enhancement :
		
		
			Using Ansible, Deploy the Manifest to Kubernetes Master and Execute Kubectl commands as mentioned below :


			--> Create a Pipeline Deployment Stage using Ansible
			
					- To copy the kubernetes manifest file - *.yaml file from jenkins_Slave Node to Kubernetes Master Node.
					
					- To execute a command : kubectl apply -f kubernetesdeploy.yaml
					
					
			--> Using Prometheus & Grafana capture the Pods Execution status and Resource utilization using Email Alerts/Notifications
					
			
			
		
			GITOps Approach :::
			
			Scenario1 :

				Pipeline1 :
				Source_Code Repo :
				
				- SCM-Checkout --> Create New_Test_Server(Terraform) --> Configure New_Test_Server(Ansible)




DevOps ::::
	
		CICD Pipeline:
		
		Scenario1 :
		
			Pipeline1 :
			
			- SCM-Checkout --> Build --> Deployed to Test Server 
			

		Scenario2 :

			Pipeline1 :
			Source_Code Repo :
			
			- SCM-Checkout --> Create New_Test_Server(Terraform) --> Configure New_Test_Server(Ansible)


			
			Pipeline2 :
			Source_Code Repo :
			- SCM-Checkout --> Build --> Deployed to New_Test_Server

			
		Scenario3 :
		
			Source_Code Repo :
			
				Application Source Code 
				
				Terraform Scripts 
				
				Ansible Playbooks 
				
				
			Pipeline1:
			
				SCM-Checkout --> Application_Build --> Create New_Test_Server(Terraform) --> Configure New_Test_Server(Ansible)
				
																												|
																												|
																												|
																						
																						  Deploy the changes to New_Test_Server
																						  
																						  
																												|
																												|
																												|
																												
																										Automated Testing 
																										
																												|
																												|
																												|
																												
																										Notify the User  
																										
																										Delete the New_Test_Server using Terraform Destroy 

																												|
																												|
																												|
																												
																											Deploy to Prod				
				
				
Business challenge/requirement
As soon as the developer pushes the updated code on the GIT master branch, the code should be checked out, compiled, tested, packaged and containerized. A new test-server should be provisioned using terraform and should be automaÙè∞Äcally configured using Ansible with all the required soÙè∞Åware‚Äôs and as soon as the server is available, the applicaÙè∞Äon must be deployed to the test-server automaÙè∞Äcally.
The deployment should then be tested using a test automaÙè∞Äon tool, and if the build is
Successful, Prod server must be configured with all the soÙè∞Åware it should be pushed to the prod server. All this should happen automaÙè∞Äcally and should be triggered from a push to the GitHub master branch. ConÙè∞Änuous monitoring server must be configured to monitor the test as well as prod server using Prometheus and Grafana should be configured to display a dashboard with following metrics.
1. CPU uÙè∞ÄlizaÙè∞Äon
2. Disk Space UÙè∞ÄlizaÙè∞Äon 3. Total Available Memory



Later, you need to implement ConÙè∞Änuous IntegraÙè∞Äon & ConÙè∞Änuous Deployment using following tools:
ÔÉº Git - For version control for tracking changes in the code files
ÔÉº Maven ‚Äì For ConÙè∞Änuous Build
ÔÉº Jenkins - For conÙè∞Änuous integraÙè∞Äon and conÙè∞Änuous deployment
ÔÉº Docker - For deploying containerized applicaÙè∞Äons
ÔÉº Kubernetes ‚Äì for running containerized applicaÙè∞Äon in managed cluster.
ÔÉº Ansible - ConfiguraÙè∞Äon management tools
ÔÉº Terraform - For creaÙè∞Äon of infrastructure.
ÔÉº Prometheus and Grafana ‚Äì For Automated Monitoring and Report VisualizaÙè∞Äon
This project will be about how to test the services and deploy code to dev/stage/prod etc, just on a click of buÙè∞Çon.

Docker file:
=============

FROM openjdk:17
ARG JAR_FILE=target/*.jar
COPY ${JAR_FILE} app.jar
ENTRYPOINT ["java","-jar","/app.jar"]
EXPOSE 8082

Jenkins file:
===============
pipeline {
    agent { label 'slave1' }

	environment {	
		DOCKERHUB_CREDENTIALS=credentials('dockerloginid')
	}

    stages {
        stage('SCM_Checkout') {
            steps {
                echo 'Perform SCM Checkout'
				git 'https://github.com/PL-DevOps-GenAI-0625/BankingWebApp.git'			
            }
        }
        stage('Application Build') {
            steps {
                echo 'Perform Application Build'
				sh 'mvn clean package'
            }
        }
        stage('Application Image Build') {
            steps {
                echo 'Perform Application Image Build'
				sh "docker build -t loksaieta/bankingwebapp:${BUILD_NUMBER} ."
				sh "docker tag loksaieta/bankingwebapp:${BUILD_NUMBER} loksaieta/bankingwebapp:latest"
            }
        }
        stage('Login to DockerHub') {
            steps {
                echo 'Login to DockerHub'
				sh 'echo $DOCKERHUB_CREDENTIALS_PSW | docker login -u $DOCKERHUB_CREDENTIALS_USR --password-stdin'
				
            }
        }
        stage('Push to DockerHub') {
            steps {
                echo 'Push Image to DockerHub'
				sh "docker push loksaieta/bankingwebapp:${BUILD_NUMBER}"
				sh "docker push loksaieta/bankingwebapp:latest"
				sh "docker rmi loksaieta/bankingwebapp:${BUILD_NUMBER}"
				sh "docker rmi loksaieta/bankingwebapp:latest"
            }
        }
        stage('Deploy to Kubernetes') {
            steps {
                script{
				sshPublisher(publishers: [sshPublisherDesc(configName: 'Kubernetes', transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: 'kubectl apply -f bankingdeploy.yaml', execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '.', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '*.yaml')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])
				}				
            }
        }
    }
}


kubernetes file:
=================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bankingapp-deploy
  labels:
    app: bankapp-deploy-lbl
spec:
  replicas: 3
  selector:
    matchLabels:
      app: bankapp-pod-lbl
  template:
    metadata:
      labels:
        app: bankapp-pod-lbl
    spec:
      containers:
      - name: banking-app-container
        image: loksaieta/bankingwebapp
        ports:
        - containerPort: 8082
---
apiVersion: v1
kind: Service
metadata:
  name: bankingapp-np-service
  labels:
    app: bankapp-svc-lbl
spec:
  selector:
    app: bankapp-pod-lbl

  type: NodePort
  ports:
  - nodePort: 31022
    port: 8082
    targetPort: 8082



 Dynamically:

           +-------------------------+
           |      GitHub Repo        |
           |  (BankingWebApp code)   |
           +------------+------------+
                        |
        Push code to master branch triggers
                        v
           +-------------------------+
           |        Jenkins CI       |
           |  (Pipeline Execution)  |
           +------------+------------+
                        |
     ------------------------------------------------------
     |                      |                              |
     v                      v                              v
Checkout Code           Build & Package               Build Docker Image
(Maven / npm / go)      (target/*.jar)            (Dockerfile -> Docker Hub)
                        |
                        v
               Test artifacts ready
                        |
                        v
           +-------------------------+
           | Terraform Provisioning  |
           |   (Dynamic Test/Prod)  |
           +------------+------------+
                        |
          Creates EC2 instance, VPC, subnet, SG, IGW
          Outputs public IP dynamically for Ansible
                        v
           +-------------------------+
           |    Ansible Configuration |
           |  (configure_server.yml) |
           +------------+------------+
                        |
        Installs Java, Docker, Maven, Git, firewall setup
                        |
                        v
           +-------------------------+
           |  Application Deployment |
           |    (deploy_app.yml)     |
           +------------+------------+
                        |
        Copies .jar, creates systemd service, starts app
                        |
                        v
           +-------------------------+
           | Automated Testing Stage |
           |   (pytest / mvn test)  |
           +------------+------------+
                        |
      Pass ‚Üí Terraform Destroy (test environment)
      Fail ‚Üí Notify and stop pipeline
                        |
                        v
           +-------------------------+
           |   Terraform Apply Prod  |
           |   Ansible Deploy Prod   |
           +------------+------------+
                        |
      Production server fully configured and application running
                        v
           +-------------------------+
           |      Monitoring         |
           | Prometheus + Grafana    |
           +------------+------------+
                        |
    Metrics collected via Node Exporter on all servers
    - CPU Utilization
    - Disk Usage
    - Memory Usage
                        v
           +-------------------------+
           |     Grafana Dashboard   |
           | Real-time monitoring    |
           +-------------------------+

üîπ Key Points in this Diagram

GitHub ‚Üí Jenkins: CI/CD starts automatically on every push to master branch.

Terraform: Creates infrastructure dynamically for test/prod. No manual IP edits.

Dynamic Inventory: Ansible picks up server IPs from Terraform output.

Ansible Configuration: Installs all required software, configures Docker, Java, firewall, etc.

Deploy Application: Systemd service ensures app runs continuously.

Automated Testing: Tests validate app on test server.

Destroy Test Server: After successful test, test server is destroyed to save cost.

Deploy to Prod: Reuse same playbooks for production with dynamic inventory.

Monitoring: Node Exporter collects server metrics ‚Üí Prometheus scrapes ‚Üí Grafana visualizes.



1Ô∏è‚É£ Terraform: Dynamic Environment Provisioning
======================================================

variable "environment" {
  description = "Environment to deploy (test/prod)"
  default     = "test"
}

provider "aws" {
  region = "ap-south-1"
}

# VPC
resource "aws_vpc" "main" {
  cidr_block       = "10.0.0.0/16"
  instance_tenancy = "default"

  tags = {
    Name = "${var.environment}-vpc"
    Env  = var.environment
  }
}

# Subnet
resource "aws_subnet" "main" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = var.environment == "prod" ? "10.0.2.0/24" : "10.0.1.0/24"
  map_public_ip_on_launch = true

  tags = {
    Name = "${var.environment}-subnet"
    Env  = var.environment
  }
}

# Internet Gateway
resource "aws_internet_gateway" "gw" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = "${var.environment}-igw"
    Env  = var.environment
  }
}

# Route Table
resource "aws_route_table" "main" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.gw.id
  }

  tags = {
    Name = "${var.environment}-rt"
    Env  = var.environment
  }
}

# Associate route table with subnet
resource "aws_route_table_association" "main" {
  subnet_id      = aws_subnet.main.id
  route_table_id = aws_route_table.main.id
}

# Security group
resource "aws_security_group" "main" {
  name   = "${var.environment}-sg"
  vpc_id = aws_vpc.main.id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 8082
    to_port     = 8082
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "${var.environment}-sg"
    Env  = var.environment
  }
}

# EC2 Instance
resource "aws_instance" "app_server" {
  ami           = "ami-0de53d8956e8dcf80" # Amazon Linux 2 in ap-south-1
  instance_type = "t2.micro"
  subnet_id     = aws_subnet.main.id
  key_name      = "aws2025-keypair1"
  vpc_security_group_ids = [aws_security_group.main.id]
  associate_public_ip_address = true

  tags = {
    Name = "${var.environment}-app-server"
    Env  = var.environment
  }

  # Output the public IP for Ansible dynamic inventory
  provisioner "local-exec" {
    command = "echo ${self.public_ip} > ${var.environment}_server_ip.txt"
  }
}

# Outputs
output "server_ip" {
  value = aws_instance.app_server.public_ip
}



‚úÖ What this does:

Dynamically creates test or prod environment based on var.environment.

Automatically generates public IP for the server and writes it to a file (test_server_ip.txt or prod_server_ip.txt) for Ansible.

No manual IP editing required.


2Ô∏è‚É£ Dynamic Inventory for Ansible
=======================================

inventory_dynamic.ini:

[test]
{{ lookup('file', 'test_server_ip.txt') }} ansible_user=ec2-user

[prod]
{{ lookup('file', 'prod_server_ip.txt') }} ansible_user=ec2-user


Or you can use Ansible dynamic inventory script with Terraform output JSON:

terraform output -json > terraform_output.json


Then in ansible.cfg:

[inventory]
enable_plugins = json


This ensures Ansible always gets the latest server IP.


3Ô∏è‚É£ Ansible Playbooks
=========================
Configure Server (configure_server.yml):

---
- name: Configure Server
  hosts: all
  become: true
  vars:
    app_user: ec2-user
    java_package: java-17-amazon-corretto
  tasks:
    - name: Update packages
      yum:
        name: "*"
        state: latest

    - name: Install required packages
      yum:
        name:
          - "{{ java_package }}"
          - git
          - docker
          - maven
        state: present

    - name: Start Docker service
      service:
        name: docker
        state: started
        enabled: yes

    - name: Add user to docker group
      user:
        name: "{{ app_user }}"
        groups: docker
        append: yes

Deploy App (deploy_app.yml):

---
- name: Deploy Application
  hosts: all
  become: true
  vars:
    app_dir: /opt/demo-app
    jar_file: demo-app.jar
  tasks:
    - name: Create app directory
      file:
        path: "{{ app_dir }}"
        state: directory
        owner: ec2-user
        group: ec2-user
        mode: 0755

    - name: Copy JAR file
      copy:
        src: target/{{ jar_file }}
        dest: "{{ app_dir }}/{{ jar_file }}"
        owner: ec2-user
        group: ec2-user
        mode: 0755

    - name: Create systemd service
      copy:
        dest: /etc/systemd/system/demo-app.service
        content: |
          [Unit]
          Description=Demo App
          After=network.target

          [Service]
          User=ec2-user
          ExecStart=/usr/bin/java -jar {{ app_dir }}/{{ jar_file }}
          Restart=always

          [Install]
          WantedBy=multi-user.target
      notify: Restart app service

    - name: Reload systemd
      command: systemctl daemon-reexec

    - name: Enable and start app
      service:
        name: demo-app
        state: started
        enabled: yes

  handlers:
    - name: Restart app service
      service:
        name: demo-app
        state: restarted


4Ô∏è‚É£ Jenkins Pipeline Flow (Automated)
======================================

pipeline {
    agent any
    environment { DOCKERHUB_CREDENTIALS = credentials('dockerloginid') }

    stages {
        stage('SCM Checkout') { steps { git 'https://github.com/PL-DevOps-GenAI-0625/BankingWebApp.git' } }
        stage('Build App') { steps { sh 'mvn clean package' } }
        stage('Build Docker Image') { steps { sh 'docker build -t myapp:${BUILD_NUMBER} .' } }
        stage('Push Docker Image') { steps { sh 'docker push myapp:${BUILD_NUMBER}' } }
        stage('Terraform Apply') { steps { sh 'cd infra/ && terraform init && terraform apply -auto-approve -var environment=test' } }
        stage('Configure Test Server') { steps { sh 'ansible-playbook -i inventory_dynamic.ini configure_server.yml' } }
        stage('Deploy to Test Server') { steps { sh 'ansible-playbook -i inventory_dynamic.ini deploy_app.yml' } }
        stage('Run Tests') { steps { sh 'pytest tests/automation/' } }
        stage('Terraform Destroy Test Server') { steps { sh 'cd infra/ && terraform destroy -auto-approve -var environment=test' } }
        stage('Deploy to Prod') { steps { sh 'terraform apply -auto-approve -var environment=prod && ansible-playbook -i inventory_dynamic.ini deploy_app.yml' } }
    }
}




5Ô∏è‚É£ Monitoring with Prometheus & Grafana
======================================

Install node_exporter on all servers via Ansible:


- name: Install node_exporter
  hosts: all
  become: true
  tasks:
    - name: Download node_exporter
      get_url:
        url: https://github.com/prometheus/node_exporter/releases/download/v1.7.2/node_exporter-1.7.2.linux-amd64.tar.gz
        dest: /tmp/node_exporter.tar.gz

    - name: Extract node_exporter
      unarchive:
        src: /tmp/node_exporter.tar.gz
        dest: /opt/
        remote_src: yes

    - name: Create systemd service
      copy:
        dest: /etc/systemd/system/node_exporter.service
        content: |
          [Unit]
          Description=Node Exporter
          After=network.target

          [Service]
          ExecStart=/opt/node_exporter-1.7.2.linux-amd64/node_exporter
          Restart=always

          [Install]
          WantedBy=multi-user.target

    - name: Enable & start node_exporter
      service:
        name: node_exporter
        state: started
        enabled: yes


Configure Prometheus to scrape metrics from all servers and Grafana to visualize:

CPU utilization

Disk space

Memory usage


‚úÖ Key Benefits of This Setup
=======================================
Fully automated CI/CD pipeline triggered by GitHub push.

Terraform provisions dynamic test/prod environments.

Ansible handles configuration and deployment automatically.

Docker images are built and pushed automatically.

Kubernetes deployment optional if needed.

Automated monitoring via Prometheus & Grafana.

No manual IP editing, fully dynamic inventory.
