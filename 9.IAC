
#########################
Day 25 : 21st August 2025
#########################		

	Infra-Structure As Code! --> IAC
	
		--> Infra-Structure Provisioning/Creation 	===> Terraform/Cloudformation/ARM
			
		--> Infra-Structure Configurations			===> Ansible/Chef/Puppet			
		
		Ansible 	==> Configuration Management Tool 
		
		Terraform 	==> Provisioning/Creation Tool		
		
		How to Create VMs ???
		
			- AWS Console! ==> 1
			
			- Servers --> 50 Developers Joined the Team! 
							
							- Provision the Workstation 
							- Install the Required Software!
							
			- CICD	  	--> Continuous Testing -->
			
						--> 200 Test Environments/Servers!
						
	
						
	- Ansible - Configuration Management Tool :::
	
		- What is Ansible ?
          Infra-Structure Configurations tool

		- Why Ansible ?
          1. Meant for configuration management and apart from this, capable for continous delivery and deployement
           In one scneario, We hav used publish puglin we have deployed from build to target while using jenkins, What if we are not using jenkins
           That's where ansible used to copy from one file to another server.capable for server provisioning but not matured as terraform.
          2. How your IT support team install all your packages to the remote machines of 100 +servers. They do it by ansible to aautomate

          3. In another scenario, 

		- Ansible Architecture :::
			- Master/Ansible Controller 
			- Client/Ansible Nodes Architecture
-
			                                                                    server1
_____Inventory file__
|
|
|  172.31.93.110              Server-x ---------ssh for operations----Server2
|  172.31.93.120             172.31.93.81
|  172.31.93.130                                                      Server3
|
|______________________


          - all the default configuration is defined in Ansible.cfg
          - inventory consits of all the targets ip adress
          -  Ansible Engine/Controller/ Manager
          -o
		- Ansible Architecture Components
			
			- Ansible Controller 
				- Install Ansible 
				- Maintain the Ansible Modules and Inventory file 
				
			- Ansible Nodes :
				- Target Machines/Clients that can be configured through Ansible Controller 
				
			- Inventory file :
				- Used to define the Ansible Nodes details
				- Host Name, User Name and Credentials - SSH Keys / Password 

              inventory
         defualt path -    /etc/ansible/hosts
        
               server1.company.com
               server2.company.com
               [mail]
               server3.company.com 
               server4.company.com
               [db]
               server5.company.com
               server6.company.com
               [web]
               server7.company.com
               server8.company.com



			- Ansible Modules :
				- It is a script or function that gets injected into the target machine.
				- These Modules are based of Python Scripts 
				- Ansible Inject/push the modules from the Ansible Controller Machine into the Ansible Nodes, 
                  perform the desired Task, and Once the Task is over, it will delete that module from that target machine. 
				
			- Ansible Config File :
			
				- Is used to define the default properties of Ansible 				
				
				
			Ansible_Controller 									Ansible_Node1 
			
				setup_Java_BuildServer.sh		==============>	Copy setup_Java_BuildServer.sh and Execute this script in Ansible Node1
					apt install git 
					apt install jdk
					apt install maven
							
							
				- Ansible Use the Push Mechanism 
				
				
			Ansible is Agentless.
				
					
- Working with Ansible Adhoc Commands & Playbooks - *.yaml Scripts
		
	- Ansible Adhoc Commands :
	
		- It is used to execute a module in the target machine.	
		
		
			- apt install git 
	
	- Ansible Playbooks :::
	
		- Used to execute the series of task/Modules in the target machine.
		- Ansible Playbooks should be reusable 
		- Ansible Playbooks are written using *.yaml Scripts 
		
			- apt Install git 			
			- apt install jdk11			
			- apt install maven 
			
	- Work with Ansible ::
	
	
		- Install and Configure Ansible Architecture :
		
			- Ansible Controller (VM)	
			
				- Ansible Node1 (VM)
				- Ansible Node2 (VM)


Ansible Installation & Configurations:

Launch - Ubuntu - v22.04 | 3 EC2 Instances... 1 for Ansible Controller & 2 as Nodes - ubuntu v22.04

SSH connections :::

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Login to Ansible Node1 & Node2. Perform below activities:

#Add User in Ansible Nodes : 

sudo -i

apt update -y 

useradd ansibleadmin -s /bin/bash -m -d /home/ansibleadmin 

passwd ansibleadmin

#Enter New Password:
#Confirm Password:

#Goto:

vi /etc/ssh/sshd_config

#Enable Password Authentication to Yes and save the file
#Execute Below command to update the changes.

service ssh reload

#As a root user edit below file:

$ visudo

#add the below mentioned line in the file and save it.
 
ansibleadmin ALL=(ALL) NOPASSWD: ALL

su - ansibleadmin

ls -a 



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#Login to Ansible Controller:
https://docs.ansible.com/ansible/latest/installation_guide/installation_distros.html

https://docs.ansible.com/ansible/latest/installation_guide/installation_distros.html#installing-ansible-on-ubuntu

sudo -i

#ssh-copy-id ansibleadmin@172.31.2.214
#ssh-copy-id ansibleadmin@172.31.33.217

sudo apt update -y

sudo apt install software-properties-common -y
sudo add-apt-repository --yes --update ppa:ansible/ansible
sudo apt update -y
sudo apt install ansible -y

ansible --version

#go to /etc/ansible

#hosts - Default inventory file
#config
#roles 

#Add User in Ansible Controller : 

useradd devopsadmin -s /bin/bash -m -d /home/devopsadmin

#useradd devopsadmin

su - devopsadmin	

#ssh-keygen -t rsa -b 1024 -m PEM

#ssh-keygen -R rsa -b 1024 -m PEM
ssh-keygen -t ecdsa -b 521										#ubuntu 22.04 or higher version of ubuntu				

ls ~/.ssh 

#You should see following two files:

#id_ecdsa - private key
#id_ecdsa.pub - public


#Goto Node1&2, within ansibleadmin home directory, create .ssh directory

vi authorized_keys

#paste the id_ecdsa.pub of devopsadmin user from controller machine to authorized_keys file in Ansible Node1 

chmod 600 /home/ansibleadmin/.ssh/*


# Use the private IP addr. of AN1 and AN2


ssh ansibleadmin@172.31.43.198
ssh ansibleadmin@172.31.38.32

chown -R devopsadmin:devopsadmin /etc/ansible

#chmod 600 /home/ansibleadmin/.ssh/*


###update vi etc/ansible/hosts


[testnodes]
samplenode1 ansible_ssh_host=172.31.43.198 ansible_ssh_user=ansibleadmin
samplenode2 ansible_ssh_host=172.31.38.32 ansible_ssh_user=ansibleadmin

[DB_Servers]
devnode1 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode2 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode3 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode4 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode5 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode6 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin

[Test_Servers] 
testnode1 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
testnode2 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin


Eg.::: 

samplenode3 ansible_ssh_host=172.31.38.135 ansible_ssh_user=ansibleadmin
samplenode4 ansible_ssh_host=172.31.38.39 ansible_ssh_user=ansibleadmin

[devnodes]
devnode1 ansible_ssh_host=172.31.5.190 ansible_ssh_user=ansibleadmin
devnode2 ansible_ssh_host=172.31.6.228 ansible_ssh_user=ansibleadmin


[webappservers]
webappserver1 ansible_ssh_host=172.31.9.49 ansible_ssh_user=ansibleadmin
webappserver2 ansible_ssh_host=172.31.9.209 ansible_ssh_user=ansibleadmin
webappserver3 ansible_ssh_host=172.31.9.209 ansible_ssh_user=ansibleadmin


#**************************************************************************************************************************
#hosts file is the default Inventory file for ansible 
#**************************************************************************************************************************
#Access thru Ansible Controller :
#**************************************************************************************************************************

ansible <hosts_name> -m <module_name> -i <inventory_file>

ansible testnodes -m ping 

ansible devnodes -m ping 


ansible dev_server_grp1 -m ping -i dev_servers

#host machines can be identified using :
# all | group_name | individual_host_name


[testnodes]
samplenode1 ansible_ssh_host=172.31.43.198 ansible_ssh_user=ansibleadmin
samplenode2 ansible_ssh_host=172.31.38.32 ansible_ssh_user=ansibleadmin




- Login to aws console
- click on EC2
- click on instances, Launch instance, Name- PL-Ansible
- AMI - ubuntu 22.04 version
- instance type t2micro
- all the parameters as default
- mention instances as 3 and click on launch

- Name one instance as Ansible -Controller and other twos as Ansible nodes

- login to Ansible controller
- sudo -i
- apt update -y


- login to Ansible node1 
=========================
- sudo -i
- apt update -y
- useradd ansibleadmin -s /bin/bash -m -d /home/ansibleadmin 
- passwd ansibleadmin
New password: 
Rtype New password: 
- goto vi /etc/ssh/sshd_config
-
#Enable Password Authentication to Yes and save the file
#Execute Below command to update the changes.
- service ssh reload
-#As a root user edit below file:
$ visudo
-
#add the below mentioned line in the file and save it.
ansibleadmin ALL=(ALL) NOPASSWD: ALL

- su - ansibleadmin
- ls -a 


- login to Ansible node2 
==========================
- sudo -i
- apt update -y
- useradd ansibleadmin -s /bin/bash -m -d /home/ansibleadmin 
- passwd ansibleadmin
New password: 
Rtype New password: 
- goto vi /etc/ssh/sshd_config
-
#Enable Password Authentication to Yes and save the file
#Execute Below command to update the changes.
- service ssh reload
-#As a root user edit below file:
$ visudo
-
#add the below mentioned line in the file and save it.
ansibleadmin ALL=(ALL) NOPASSWD: ALL

- su - ansibleadmin
- ls -a 
- pwd
/home/ansibleadmin
- Now go to ansible controller




Back to ansible controller:
------------------------------
-sudo apt install software-properties-common -y
 sudo add-apt-repository --yes --update ppa:ansible/ansible
 sudo apt update -y
 sudo apt install ansible -y
- ansible --version
- cd /etc/ansible
ansible.cfg  hosts roles

- #Add User in Ansible Controller : 
  useradd devopsadmin -s /bin/bash -m -d /home/devopsadmin
-
#useradd devopsadmin
su - devopsadmin	
- 
#ssh-keygen -t rsa -b 1024 -m PEM
#ssh-keygen -R rsa -b 1024 -m PEM
ssh-keygen -t ecdsa -b 521										#ubuntu 22.04 or higher version of ubuntu				

- 
ls ~/.ssh 

#You should see following two files:

#id_ecdsa - private key
#id_ecdsa.pub - public

-
#Goto Node1&2, within ansibleadmin home directory, create .ssh directory
- pwd
- /home/ansibleadmin
- mkdir .ssh
- cd .ssh
- vi authorized_keys
-
#paste the id_ecdsa.pub of devopsadmin user from controller machine to authorized_keys file in Ansible Node1 
# same activity in node2
chmod 600 /home/ansibleadmin/.ssh/*

- Login back to ansible controller
# Use the private IP addr. of AN1 and AN2

ssh ansibleadmin@172.31.43.198
ssh ansibleadmin@172.31.38.32


- chown -R devopsadmin:devopsadmin /etc/ansible

#chmod 600 /home/ansibleadmin/.ssh/*


###update 
-vi etc/ansible/hosts

[testnodes]
samplenode1 ansible_ssh_host=172.31.43.198 ansible_ssh_user=ansibleadmin
samplenode2 ansible_ssh_host=172.31.38.32 ansible_ssh_user=ansibleadmin

[DB_Servers]
devnode1 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode2 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode3 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode4 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode5 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode6 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin

[Test_Servers] 
testnode1 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
testnode2 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin

- :wq!
- 
Eg.::: 

samplenode3 ansible_ssh_host=172.31.38.135 ansible_ssh_user=ansibleadmin
samplenode4 ansible_ssh_host=172.31.38.39 ansible_ssh_user=ansibleadmin

[devnodes]
devnode1 ansible_ssh_host=172.31.5.190 ansible_ssh_user=ansibleadmin
devnode2 ansible_ssh_host=172.31.6.228 ansible_ssh_user=ansibleadmin


[webappservers]
webappserver1 ansible_ssh_host=172.31.9.49 ansible_ssh_user=ansibleadmin
webappserver2 ansible_ssh_host=172.31.9.209 ansible_ssh_user=ansibleadmin
webappserver3 ansible_ssh_host=172.31.9.209 ansible_ssh_user=ansibleadmin


#**************************************************************************************************************************
#hosts file is the default Inventory file for ansible 
#**************************************************************************************************************************
#Access thru Ansible Controller :
#**************************************************************************************************************************

ansible <hosts_name> -m <module_name> -i <inventory_file>

ansible testnodes -m ping 

ansible devnodes -m ping 


ansible dev_server_grp1 -m ping -i dev_servers

#host machines can be identified using :
# all | group_name | individual_host_name


[testnodes]
samplenode1 ansible_ssh_host=172.31.43.198 ansible_ssh_user=ansibleadmin
samplenode2 ansible_ssh_host=172.31.38.32 ansible_ssh_user=ansibleadmin

- ansible testnodes -m ping
- ansible samplenode1 -m ping 
- ansible all -m ping
- ansible dev_server_grp1 -m ping -i dev_servers




#########################
Day 26 : 22nd August 2025
#########################	


*******************
ansible all -m ping ## will ping all hosts from /etc/ansible/hosts file

ansible samplenode1 -m ping
ansible samplenode2 -m ping

#or using user defined Inventory file
#ansible ansible-node1 -m ping -i myinventoryfile.txt

ansible samplenode1 -m ping -i myinventoryfile.txt

ansible samplenode2 -m ping
ansible samplenode1 -m shell -a "sleep 20 ; echo 'hi'"


Ansible Facts!

ansible samplenode1 -m setup
ansible samplenode1 -m setup -a "filter=ansible_mounts"

ansible testnodes -m setup -a "filter=ansible_distribution"

## Transfer a file from Ansible Controller to Target Nodes using copy Module

ansible samplenode1 -m copy -a "src=/etc/ansible/file1.txt dest=/home/ansibleadmin"

ansible samplenode1 -m copy -a "src=/etc/ansible/s1.txt dest=/home/ansibleadmin bacup =yes"


## Transfers a file  Ansible  nodes to  Ansible Controller using FETCH module

ansible samplenode1 -m fetch -a "src=/home/ansibleadmin/filefrom _AN1.txt dest=/home/devopsadmin"


Ansible_Controller :

	-- AN1 - Jenkins_Slave		== *.war
	
	-- AN2 - Tomcat_Server 		== *.war


UseCase 1 :
	- Deploy an artifact *.war from Jenkins Slave Machine to QA-Server.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


https://docs.ansible.com/ansible/2.9/modules/list_of_all_modules.html



### List all modules:
ansible-doc -l

### No of modules:
ansible-doc -l | wc -l

##Seach for specific modules:
ansible-doc -l | grep shel

### To know about any specific modules:
ansible-doc shell


https://docs.ansible.com/ansible/2.9/modules/list_of_all_modules.html



******************
				
- Login back to Ansible controller  and Ansible nodes
- In Ansible controller
- cd /etc/ansible
- ls
ansible.cfg dev_servers hosts roles
-  ansible samplenode1 -m shell -a "sleep 20 ; echo 'hi'"
-  ansible samplenode1 -m setup  ( To know the architecutre details of particular node from ansible cntroller)
-  ansible samplenode1 -m setup -a "filter=ansible_mounts"
-  ansible testnodes -m setup -a "filter=ansible_distribution"
- echo "rec1" >> file1.txt
- pwd
/etc/ansible
- ls
ansible.cfg dev_servers hosts roles file1.txt
- ansible samplenode1 -m copy -a "src=/etc/ansible/file1.txt dest=/home/ansibleadmin"
- echo "dummy-record1" > file1.txt
- ansible samplenode1 -m copy -a "src=/etc/ansible/s1.txt dest=/home/ansibleadmin backup =yes"
- If you check now in samplenode1, loging to samplenod1
- ls ( you can see the backup as well in another file contains of old record)
file1.txt file1.txt.125464.2025-08-22@14:55:51
- here echo "rec1" >> filefrom_ANI.txt
- ls
- file1.txt file1.txt.125464.2025-08-22@14:55:51 
- Now got to ansible controller
- ansible samplenode1 -m fetch -a "src=/home/ansibleadmin/filefrom_AN1.txt dest=/home/devopsadmin"


- ansible-doc -l ( use to get list of modules********)



			- Ansible Adhoc Commands :
			
				- It is used to execute a module in the target machine.
				
					Eg.: 
					
					Target Server :
					
						install git :
							- apt install git -y				
			
			- Ansible Playbooks :
			
				- Used to execute the series of task/Modules in the target machine.
				- Ansible Playbooks should be reusable 
				- Ansible Playbooks are written using *.yaml Scripts 
				
					Eg.: 
						Target Server : 
						
							Setup the Jenkins Slave_Node :
							
								- apt install git -y 
								- apt install jdk -y 
								- apt install maven -y 
								- apt install docker.io -y 
	
	
*******************************************************************


#**************************************************************************************************************************
Ansible Playbooks :::
#**************************************************************************************************************************
*.yaml scripts 		==> Key-value pair

- Playbook creation



Playbooks :::::

	- *.yaml Script
	- Defines the Tasks/Modules to be executed in target nodes.
	- Reusability
	
	
Build Server :::

	- install jdk 
	- install git 
	- install maven



#Ansible Variables !

#shell: echo $var1"

in yaml: "{{var1}}"

ansible samplenode1 -m setup

dobug-
   msg  -used to print the constant  value and variable data
   var - used to print only the fariable data

key: value pair



- In Ansible controller 
- cd /etc/ansible
- ls
ansible.cfg dev_servers file1.txt hosts roles
- mkdir playbooks
- vi debugmod.yaml
paste the below content
---
 - hosts: testnodes
   tasks:
   - debug:
      msg:
       - "The os distributing is: {{ansiblre_distribution)"
       - "The os name is: {{ansible_system}}"
       - "The os familu is: {{ansible_os_faamily}}"

- :wq!
-  ansible-playbook dbug.yaml




- vi  var1.yaml

---
 - hosts: samplenode1
   vars:
    x: 23
    my_num: 45.67
    my_name: Loksai
    my_b: YES
   tasks:
   - debug:
      msg:
       - "The value of x is: {{x}}"
       - "The value of my_num: {{my_num}}"
       - "The value of my_name: {{my_name}}"
       - "The value of my_b is: {{my_b}}"

- ansible-playbook var1.yaml


- mkdir variables
- cd variables
- vi myvarfile1.yaml

/etc/ansible/variables/myvarfile1.yaml
    x:26
    my_num: 45.00067
    my_name: Loksai_ETA
    my_b: YES

- cd ..
/etc/ansible/
- cd playbooks
- vi var2.yaml
---
 - hosts: samplenode1
   vars_files:
     - /etc/ansible/variables/myvarfile1.yaml
   tasks:
   - debug:
      msg:
       - "The value of x is: {{x}}"
       - "The value of my_num: {{my_num}}"
       - "The value of my_name: {{my_name}}"
       - "The value of my_b is: {{my_b}}"

- ansible-playbook var1.yaml





-- Login to ansible node1
-- sudo -i
-- cd /etc/ansible/playbooks/
 

####################################################################################
---
 - hosts: samplenode1
   gather_facts: false
   become: yes
   tasks:
   - name: Manage nginx tool
     apt:
       name: nginx
       state: present
	   

Eg:
	name : nginx
	
	state : present/absent/latest
	
	sudo apt install nginx			# In Linux Ubuntu 
	
	sudo apt remove nginx			# In Linux Ubuntu 
******************************************************************

-- vi var3.yaml
#var3.yaml
---
 - hosts: "{{ host_name }}"
   become: yes
   tasks:
   - name: Manage "{{ tool_name }}" service
     apt:
       name: "{{ tool_name }}"
       state: "{{ tool_state }}"


ansible-playbook var3.yaml -e "host_name=testnodes tool_name=nginx tool_state=present"



Handling Variables in Ansible :::

	- Environment variables -- accessible using Setup Module.
	
	- Hardcoding the values in playbook 
		using vars :
	
	- Using External Variable files :
		using var_files

	- Using -e to pass the values at run time.


Handling Variables :::

	--> Hardcoding the variables --> vars:
	--> Using Variables files    --> vars_files: 	#recommended approach
	--> Passing Extra values     --> -e 



Using When Condition :::

---
  - hosts: testnodes
    become: yes
    tasks:
      - name: Install-nginx on Debian
        apt:
          name: nginx
          state: "{{pkg_state}}"
        when: ansible_os_family == "Debian"
      - name: Install-nginx on Redhat
        yum:
          name: nginx
          state: "{{pkg_state}}"
        when: ansible_os_family == "RedHat"



1:48:55

# 1. HostPath YAML file

apiVersion: v1
kind: Pod
metadata:
  name: nginx-hostpath
spec:
  containers:
    - name: nginx-container
      image: nginx
      volumeMounts:
      - mountPath: /test-mnt
        name: test-vol
  volumes:
  - name: test-vol
    hostPath:
      path: /test-vol


	--mount source=vol1,destination=/vol1

*******************************************************************
# 2. Create and Display HostPath

kubectl create -f nginx-hostpath.yaml
kubectl get po
kubectl exec nginx-hostpath df /test-mnt

*******************************************************************
3. Test: Creating "test" file underlying host dir & accessing from from pod

From HOST:
~~~~~~~~~~
cd /test-vol
echo "From Host" > from-host.txt
cat from-host.txt

From POD:
~~~~~~~~
kubectl exec nginx-hostpath cat /test-mnt/from-host.txt


*******************************************************************
4. Test: Creating "test" file inside the POD & accessing from underlying host dir

From POD:
~~~~~~~~~
kubectl exec nginx-hostpath -it bash
cd /test-mnt
echo "From Pod" > from-pod.txt
cat from-pod.txt

From Host:
~~~~~~~~~~
cd /test-vol
ls
cat from-pod.txt


*******************************************************************
5. Clean up

kubectl delete po nginx-hostpath
kubectl get po
ls /test-vol


******************************************************************* 
