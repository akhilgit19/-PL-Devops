
#########################
Day 25 : 21st August 2025
#########################		

	Infra-Structure As Code! --> IAC
	
		--> Infra-Structure Provisioning/Creation 	===> Terraform/Cloudformation/ARM
			
		--> Infra-Structure Configurations			===> Ansible/Chef/Puppet			
		
		Ansible 	==> Configuration Management Tool 
		
		Terraform 	==> Provisioning/Creation Tool		
		
		How to Create VMs ???
		
			- AWS Console! ==> 1
			
			- Servers --> 50 Developers Joined the Team! 
							
							- Provision the Workstation 
							- Install the Required Software!
							
			- CICD	  	--> Continuous Testing -->
			
						--> 200 Test Environments/Servers!
						
	
						
	- Ansible - Configuration Management Tool :::
	
		- What is Ansible ?
          Infra-Structure Configurations tool

		- Why Ansible ?
          1. Meant for configuration management and apart from this, capable for continous delivery and deployement
           In one scneario, We hav used publish puglin we have deployed from build to target while using jenkins, What if we are not using jenkins
           That's where ansible used to copy from one file to another server.capable for server provisioning but not matured as terraform.
          2. How your IT support team install all your packages to the remote machines of 100 +servers. They do it by ansible to aautomate

          3. In another scenario, 

		- Ansible Architecture :::
			- Master/Ansible Controller 
			- Client/Ansible Nodes Architecture
-
			                                                                    server1
_____Inventory file__
|
|
|  172.31.93.110              Server-x ---------ssh for operations----Server2
|  172.31.93.120             172.31.93.81
|  172.31.93.130                                                      Server3
|
|______________________


          - all the default configuration is defined in Ansible.cfg
          - inventory consits of all the targets ip adress
          -  Ansible Engine/Controller/ Manager
          -o
		- Ansible Architecture Components
			
			- Ansible Controller 
				- Install Ansible 
				- Maintain the Ansible Modules and Inventory file 
				
			- Ansible Nodes :
				- Target Machines/Clients that can be configured through Ansible Controller 
				
			- Inventory file :
				- Used to define the Ansible Nodes details
				- Host Name, User Name and Credentials - SSH Keys / Password 

              inventory
         defualt path -    /etc/ansible/hosts
        
               server1.company.com
               server2.company.com
               [mail]
               server3.company.com 
               server4.company.com
               [db]
               server5.company.com
               server6.company.com
               [web]
               server7.company.com
               server8.company.com



			- Ansible Modules :
				- It is a script or function that gets injected into the target machine.
				- These Modules are based of Python Scripts 
				- Ansible Inject/push the modules from the Ansible Controller Machine into the Ansible Nodes, 
                  perform the desired Task, and Once the Task is over, it will delete that module from that target machine. 
				
			- Ansible Config File :
			
				- Is used to define the default properties of Ansible 				
				
				
			Ansible_Controller 									Ansible_Node1 
			
				setup_Java_BuildServer.sh		==============>	Copy setup_Java_BuildServer.sh and Execute this script in Ansible Node1
					apt install git 
					apt install jdk
					apt install maven
							
							
				- Ansible Use the Push Mechanism 
				
				
			Ansible is Agentless.
				
					
- Working with Ansible Adhoc Commands & Playbooks - *.yaml Scripts
		
	- Ansible Adhoc Commands :
	
		- It is used to execute a module in the target machine.	
		
		
			- apt install git 
	
	- Ansible Playbooks :::
	
		- Used to execute the series of task/Modules in the target machine.
		- Ansible Playbooks should be reusable 
		- Ansible Playbooks are written using *.yaml Scripts 
		
			- apt Install git 			
			- apt install jdk11			
			- apt install maven 
			
	- Work with Ansible ::
	
	
		- Install and Configure Ansible Architecture :
		
			- Ansible Controller (VM)	
			
				- Ansible Node1 (VM)
				- Ansible Node2 (VM)


Ansible Installation & Configurations:

Launch - Ubuntu - v22.04 | 3 EC2 Instances... 1 for Ansible Controller & 2 as Nodes - ubuntu v22.04

SSH connections :::

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Login to Ansible Node1 & Node2. Perform below activities:

#Add User in Ansible Nodes : 

sudo -i

apt update -y 

useradd ansibleadmin -s /bin/bash -m -d /home/ansibleadmin 

passwd ansibleadmin

#Enter New Password:
#Confirm Password:

#Goto:

vi /etc/ssh/sshd_config

#Enable Password Authentication to Yes and save the file
#Execute Below command to update the changes.

service ssh reload

#As a root user edit below file:

$ visudo

#add the below mentioned line in the file and save it.
 
ansibleadmin ALL=(ALL) NOPASSWD: ALL

su - ansibleadmin

ls -a 



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#Login to Ansible Controller:
https://docs.ansible.com/ansible/latest/installation_guide/installation_distros.html

https://docs.ansible.com/ansible/latest/installation_guide/installation_distros.html#installing-ansible-on-ubuntu

sudo -i

#ssh-copy-id ansibleadmin@172.31.2.214
#ssh-copy-id ansibleadmin@172.31.33.217

sudo apt update -y

sudo apt install software-properties-common -y
sudo add-apt-repository --yes --update ppa:ansible/ansible
sudo apt update -y
sudo apt install ansible -y

ansible --version

#go to /etc/ansible

#hosts - Default inventory file
#config
#roles 

#Add User in Ansible Controller : 

useradd devopsadmin -s /bin/bash -m -d /home/devopsadmin

#useradd devopsadmin

su - devopsadmin	

#ssh-keygen -t rsa -b 1024 -m PEM

#ssh-keygen -R rsa -b 1024 -m PEM
ssh-keygen -t ecdsa -b 521										#ubuntu 22.04 or higher version of ubuntu				

ls ~/.ssh 

#You should see following two files:

#id_ecdsa - private key
#id_ecdsa.pub - public


#Goto Node1&2, within ansibleadmin home directory, create .ssh directory

vi authorized_keys

#paste the id_ecdsa.pub of devopsadmin user from controller machine to authorized_keys file in Ansible Node1 

chmod 600 /home/ansibleadmin/.ssh/*


# Use the private IP addr. of AN1 and AN2


ssh ansibleadmin@172.31.43.198
ssh ansibleadmin@172.31.38.32

chown -R devopsadmin:devopsadmin /etc/ansible

#chmod 600 /home/ansibleadmin/.ssh/*


###update vi etc/ansible/hosts


[testnodes]
samplenode1 ansible_ssh_host=172.31.43.198 ansible_ssh_user=ansibleadmin
samplenode2 ansible_ssh_host=172.31.38.32 ansible_ssh_user=ansibleadmin

[DB_Servers]
devnode1 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode2 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode3 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode4 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode5 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode6 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin

[Test_Servers] 
testnode1 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
testnode2 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin


Eg.::: 

samplenode3 ansible_ssh_host=172.31.38.135 ansible_ssh_user=ansibleadmin
samplenode4 ansible_ssh_host=172.31.38.39 ansible_ssh_user=ansibleadmin

[devnodes]
devnode1 ansible_ssh_host=172.31.5.190 ansible_ssh_user=ansibleadmin
devnode2 ansible_ssh_host=172.31.6.228 ansible_ssh_user=ansibleadmin


[webappservers]
webappserver1 ansible_ssh_host=172.31.9.49 ansible_ssh_user=ansibleadmin
webappserver2 ansible_ssh_host=172.31.9.209 ansible_ssh_user=ansibleadmin
webappserver3 ansible_ssh_host=172.31.9.209 ansible_ssh_user=ansibleadmin


#**************************************************************************************************************************
#hosts file is the default Inventory file for ansible 
#**************************************************************************************************************************
#Access thru Ansible Controller :
#**************************************************************************************************************************

ansible <hosts_name> -m <module_name> -i <inventory_file>

ansible testnodes -m ping 

ansible devnodes -m ping 


ansible dev_server_grp1 -m ping -i dev_servers

#host machines can be identified using :
# all | group_name | individual_host_name


[testnodes]
samplenode1 ansible_ssh_host=172.31.43.198 ansible_ssh_user=ansibleadmin
samplenode2 ansible_ssh_host=172.31.38.32 ansible_ssh_user=ansibleadmin




- Login to aws console
- click on EC2
- click on instances, Launch instance, Name- PL-Ansible
- AMI - ubuntu 22.04 version
- instance type t2micro
- all the parameters as default
- mention instances as 3 and click on launch

- Name one instance as Ansible -Controller and other twos as Ansible nodes

- login to Ansible controller
- sudo -i
- apt update -y


- login to Ansible node1 
=========================
- sudo -i
- apt update -y
- useradd ansibleadmin -s /bin/bash -m -d /home/ansibleadmin 
- passwd ansibleadmin
New password: 
Rtype New password: 
- goto vi /etc/ssh/sshd_config
-
#Enable Password Authentication to Yes and save the file
#Execute Below command to update the changes.
- service ssh reload
-#As a root user edit below file:
$ visudo
-
#add the below mentioned line in the file and save it.
ansibleadmin ALL=(ALL) NOPASSWD: ALL

- su - ansibleadmin
- ls -a 


- login to Ansible node2 
==========================
- sudo -i
- apt update -y
- useradd ansibleadmin -s /bin/bash -m -d /home/ansibleadmin 
- passwd ansibleadmin
New password: 
Rtype New password: 
- goto vi /etc/ssh/sshd_config
-
#Enable Password Authentication to Yes and save the file
#Execute Below command to update the changes.
- service ssh reload
-#As a root user edit below file:
$ visudo
-
#add the below mentioned line in the file and save it.
ansibleadmin ALL=(ALL) NOPASSWD: ALL

- su - ansibleadmin
- ls -a 
- pwd
/home/ansibleadmin
- Now go to ansible controller




Back to ansible controller:
------------------------------
-sudo apt install software-properties-common -y
 sudo add-apt-repository --yes --update ppa:ansible/ansible
 sudo apt update -y
 sudo apt install ansible -y
- ansible --version
- cd /etc/ansible
ansible.cfg  hosts roles

- #Add User in Ansible Controller : 
  useradd devopsadmin -s /bin/bash -m -d /home/devopsadmin
-
#useradd devopsadmin
su - devopsadmin	
- 
#ssh-keygen -t rsa -b 1024 -m PEM
#ssh-keygen -R rsa -b 1024 -m PEM
ssh-keygen -t ecdsa -b 521										#ubuntu 22.04 or higher version of ubuntu				

- 
ls ~/.ssh 

#You should see following two files:

#id_ecdsa - private key
#id_ecdsa.pub - public

-
#Goto Node1&2, within ansibleadmin home directory, create .ssh directory
- pwd
- /home/ansibleadmin
- mkdir .ssh
- cd .ssh
- vi authorized_keys
-
#paste the id_ecdsa.pub of devopsadmin user from controller machine to authorized_keys file in Ansible Node1 
# same activity in node2
chmod 600 /home/ansibleadmin/.ssh/*

- Login back to ansible controller
# Use the private IP addr. of AN1 and AN2

ssh ansibleadmin@172.31.43.198
ssh ansibleadmin@172.31.38.32


- chown -R devopsadmin:devopsadmin /etc/ansible

#chmod 600 /home/ansibleadmin/.ssh/*


###update 
-vi etc/ansible/hosts

[testnodes]
samplenode1 ansible_ssh_host=172.31.43.198 ansible_ssh_user=ansibleadmin
samplenode2 ansible_ssh_host=172.31.38.32 ansible_ssh_user=ansibleadmin

[DB_Servers]
devnode1 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode2 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode3 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode4 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode5 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
devnode6 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin

[Test_Servers] 
testnode1 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin
testnode2 ansible_ssh_host=172.31.8.78 ansible_ssh_user=ansibleadmin

- :wq!
- 
Eg.::: 

samplenode3 ansible_ssh_host=172.31.38.135 ansible_ssh_user=ansibleadmin
samplenode4 ansible_ssh_host=172.31.38.39 ansible_ssh_user=ansibleadmin

[devnodes]
devnode1 ansible_ssh_host=172.31.5.190 ansible_ssh_user=ansibleadmin
devnode2 ansible_ssh_host=172.31.6.228 ansible_ssh_user=ansibleadmin


[webappservers]
webappserver1 ansible_ssh_host=172.31.9.49 ansible_ssh_user=ansibleadmin
webappserver2 ansible_ssh_host=172.31.9.209 ansible_ssh_user=ansibleadmin
webappserver3 ansible_ssh_host=172.31.9.209 ansible_ssh_user=ansibleadmin


#**************************************************************************************************************************
#hosts file is the default Inventory file for ansible 
#**************************************************************************************************************************
#Access thru Ansible Controller :
#**************************************************************************************************************************

ansible <hosts_name> -m <module_name> -i <inventory_file>

ansible testnodes -m ping 

ansible devnodes -m ping 


ansible dev_server_grp1 -m ping -i dev_servers

#host machines can be identified using :
# all | group_name | individual_host_name


[testnodes]
samplenode1 ansible_ssh_host=172.31.43.198 ansible_ssh_user=ansibleadmin
samplenode2 ansible_ssh_host=172.31.38.32 ansible_ssh_user=ansibleadmin

- ansible testnodes -m ping
- ansible samplenode1 -m ping 
- ansible all -m ping
- ansible dev_server_grp1 -m ping -i dev_servers


*******************************************************************
*******************************************************************

# 1. HostPath YAML file

apiVersion: v1
kind: Pod
metadata:
  name: nginx-hostpath
spec:
  containers:
    - name: nginx-container
      image: nginx
      volumeMounts:
      - mountPath: /test-mnt
        name: test-vol
  volumes:
  - name: test-vol
    hostPath:
      path: /test-vol


	--mount source=vol1,destination=/vol1

*******************************************************************
# 2. Create and Display HostPath

kubectl create -f nginx-hostpath.yaml
kubectl get po
kubectl exec nginx-hostpath df /test-mnt

*******************************************************************
3. Test: Creating "test" file underlying host dir & accessing from from pod

From HOST:
~~~~~~~~~~
cd /test-vol
echo "From Host" > from-host.txt
cat from-host.txt

From POD:
~~~~~~~~
kubectl exec nginx-hostpath cat /test-mnt/from-host.txt


*******************************************************************
4. Test: Creating "test" file inside the POD & accessing from underlying host dir

From POD:
~~~~~~~~~
kubectl exec nginx-hostpath -it bash
cd /test-mnt
echo "From Pod" > from-pod.txt
cat from-pod.txt

From Host:
~~~~~~~~~~
cd /test-vol
ls
cat from-pod.txt


*******************************************************************
5. Clean up

kubectl delete po nginx-hostpath
kubectl get po
ls /test-vol


******************************************************************* 
