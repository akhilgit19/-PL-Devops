
Cloud Computing:
==========================
Offers, Servers, storage ,databases, private networks, and numerous other services online,
eliminating the need to own any hardware or build a data center.

Benefits
=================
Intial Cost Savings

- No upfron investiment in hardware or infrastruce
Flexibility
- Choose from various platforms and services based on busines needs

Automatic updates
- Cloud providers handle software and security updates automatically.

globally distributed
- Design ensures low latency and high performance across regions

pay as yo go mode
- Only pay for the resources you use, optimizing cost

high avaiablity and redundancy
- Built in failover mechanisms ensure continious uptime

Zeor Hardware Maintenance
- No need to manage or replace physical hardware


Use Cases:
==============
- Hosting websites
- hosting entire IT environments
- E- commerce
- large data sotrage
- DR sites and backuops
- to host mobile app
- big data/Data science
- IOT (Internet of Things)


Why GCP?
============
- Superior AI and Machine Learning
- Best in class data analytics
- Multi cloud and hybrid cloud flexibliity
- Global Network Infrastructure
- Cost Effective Pricing and Discounts
- Security and Compliance
- Kubernetes, Anthos, Vmware
- sustainablility and carbon neutral cloud


Deployment Models
====================
- Public 
    - Application Services
    - platform services
    - Message services
    - Compute servcies
    - Storage services
 1.The most widely cloud model
 2.The data center is hosted at the cloud services
 3.Users can acess it via the internet
 4.The Public cloud is shared among multiple organizations
 5.They Cloud provider handles all aspects of  hardware and data center management
 6.it offers a cost effective solution with no hardeware mainteance  requuired
 7.operates on a pay as you use pricing model


- Private
 1) A private cloud is a platform build and maintained by a single organization
 2) it can be hosted on premises and accessed thorough a private network.
 3) The organization owns both the hardware and software
 4) There are upfront investment and operations costs involved.
 5) Compared to the public cloud a private cloud offers greated hardware and enhanced security.

Eg- HP, Oracle, SAP AHAN, CICO

- Hybrid

Private + Public

- community cloud:
  Community cloud is a private cloud model 
  that allows multiple organizations to 
  share the same platofrm

eg- health care ,govt websites

Service Models
====================

IaaS( Infrascture as a service)            
--------------
Applications
Data
Runtime
Middleware
O/S
Virtualizations--------------------Frome here cloud provider will take
Servers
Storage
Networking

PaaS( Platform as a service)            
--------------
Applications
Data
Runtime -------------------Frome here cloud provider will take care
Middleware
O/S
Virtualizations
Servers
Storage
Networking


SaaS Software as a service
----------------------------- Completely taken care by cloud
Applications
Data
Runtime
Middleware
O/S
Virtualizations
Servers
Storage
Networking


GCP Interfaces
================
-GCP Console:
  console.cloud.google.com
-Cloud SK and Cloud shell:
 In console.cloud.google.com you will cloud sheel
-API:
-Goodcloud API:







 IAM RESOURCES AND ACCESS 
==================================
.Google Cloud resource Hierarchy
.Policy Inheritance
.Identiy and Access Management( IAM)
.IAM roles
.Service accounts


Organization:
===================
Root Node in the google cloud resource hierarchy
Hierarchical ancestor of project resources and folders
Organization adminstrators have central control of all resources
An organization ID, Which is a unique identifier for an organzation

Organization node is the top most resources
- Everything attached to the account goes under the organization node


Folders
============

-To sub-organiazations within the organization
additional grouping mechanism and isolation boundaries between projects and  organization

-Folders can be used to model different legal entitie, deparrtments adn teams  within a company

-Each team folder could contain additional sub-folders to represent different  applications
folders acts as a policy inheritance point for Cloud IAM and organization policies


Folders can contain subfolders and projecdts

- The resources inherit policies and permission assigned to folders

Folders group projects
- Folders allow you to group resources on per daepartment basis

Folders faciliate policy inheritance
- The projects inherit policies assigned to a folder




Projects
============

Project is the base-level organizin entity

Two identifiers: Project ID and Project  number(read-only)
One mutable display name.

The lifecycle state of the project; for example, ACTIVE or DETELTE_REQUESTEDD
Projects are linked to Billing Accounts

Shut down project "My First Project"
When you shut down a project:

Charges from your previous usage may continue to appear for up to a day.
To avoid unexpected charges, disable billing for the project before shutting it down.
Traffic serving stops within a few hours.
You will no longer have access to the project.
The project will be permanently deleted after 30 days. However, some resources may be permanently deleted sooner.
The owners of the project will be notified and can stop the deletion within 30 days.


- organization- dashboard- project info- go to project settings
- project name - Paperlive-GCP
- Project id 
- Project number

Organization--------- Example.come

Folders-------
     Human Resources      Engineering     Sales

Projects
    example-dev         example-prod       example-test

Google cloudl 
services 



IAM-POLICY INHERITANCE
==========================
- IAM Policies can be set the organization folder project and resource levels
- A resource inherits policies from its parent resource.
- if a policy is set at the organization level it is inheritedby all child  folders and projects
- if a poilicy is set at the project elvel it is inherited by all child resources
- They effective  policy of a resource is the combination of the policy set on the resource and the 
  policies inherited  from its parent entities
- Resources inheri policies from their project which in turn inherits policies form the organization
- Organaiztion level policies also apply at the resource level


- organization- dashboard- project info- go to project settings
- project name - Demo-PaperLive
- Project id 
- Project number

- You can go to IAM resource  IN the VIEW BY PRINCIPALS
- You can see your id ,name and role-owner
- you can provide access by click on grant access and ,
  new prinicpals- mail id
  role-   basic  you wil lhave owner,editor browser and viewer




IAM - WHO-  mail id
      what - role- resource role
      which- which resource


IAM
- Who (identity) has what access (role) to which resource defines the fundamental structure of Cloud IAM
- Google kubernetes engine GKE clusters and cloud storage buckets are examples of google cloud resources
- permission are grouped into roles and roles are assigned to authenticated members
- A cloud IAM policy defines and enforces which roles are granted to which members and this  policy  is attached to a resource
- Organizations folder projects used to organize resources are also conisdered resources google cloud
- When an authentication member attempts to a resource, Cloud IAM checks the reourdces policy to 
  to determinte whether the action is allowed


IAM Components

Member: A member can be a google account(for end unsers), a service account( for apps and virtual machine),
a google group, or cloud identiy domain that can  access a resources.\\
The identiy of a member is an email address associated with a user,service account
or googlegroup or a domain name associated with  Gsuite or cloud identiy domains


Role:
 A role is a collection of permissions. Permission determine what 
what operatiosn are allowed when you grant a role to a member, you grant all the permission that the role contains

Policy: The Cloud IAM policy binds one or more members to a role. When you want to define who (member) has what type of access(role) 
on  a resource, you create a policy and attach it to the resource.


Service accounts are assigned roles
==========================================
                  Service account
Virtual mahcine----  Create a service--------------- cloud storage
                     account to authenticate
                     the VM to cloud storage

- To create service account

-Service Account details
- click on IAM
- click on service accoutns
- click on service acconts
- service account name- data-copy
- service account -id -data copy
- click on create and continue

Grant this servie account  access to the project

- select a role- serach for storage- select storage admin
- click on continue

- Grant users access to this service account(optiona)

- click on done
- now click on data-copy, click on 3 dots, click on managedkey, 
  click on add key, click on create key, download and copy



Lab:

- In Demo-Paperlive project
Granting Access:
- click on IAM, click on grant access
- give the gmail account
- in Assign roles- select storage admin
- click on save
- Now loging in gcp console as the another gmail account
- click on select project
- Click on All, select Demo-Paperlive project
- click on cloud storage
- click on buckets
- click on create ( This is because we got storage admin access)

- Now add one more role , go to the IAM, click on same gmail account
  click on edit principal click on add role
- In role, click on basic, select viewer access
- Now if you login with the account, you can see 

- In role, click on basic, select owner access
- Now if you login with the account, you can see  you can deploy containers





Types of file Storage:
=========================

File Storage
Block Storage
Object Storage


GCS Object Storage:
-Cloud storage provides worldwide
 hightly durable, object storage
 that scales to exabytes of data

GCS- Key features
- Highly scalable unlimited storage
- data durablity -99.99999
- data availablity- 99.99
- fast and highly avaialbe
- no minimal object size
- pay as per go model
- cost effective solution


- Object life cycle Managment
- Object Versioning
- Rention policies
- Object holds
- Customer Managed Encryption keys
- Customer supplied encruyption keys
- Uniform bucket level access ( disable object ACLS)
- Notification for event  driven workloads
- Audit logs

Google Cloud Storage Structure
------------------------------
Project- Bucket-objects

Buckets
- Logical Container which holds your data
- Every data you store will be under a bucket
- Global unique name
- Bucket Name cannot be changed once it is created
- Labels can be added for identification( upto 63 character)
- IAM provides security and permission to bucket

Objects
- Objects are actual data you save incl cloud storage
- There is no limit on number of objects in a bucket
- Max size of singel objects is 5 TB
- Recommended to have fewer bucket and more objects
- Signed URL- Time-limited resource access

- In paperlive-gcp project
- click on cloud storage resource
- click on buckets
- click on create
- name- gcp-training-feb20
- labels- clck on add a lable
- key1- dev , value -=1
- Choose where to sotre your data
 location type
  Multi- region
  dual -region-- choose
  region ( lowest latency with a single region
- location -Asia pacific
- regions - asia-south1(Mumabai) and asia-south2 (Delhi)
- Choose a storage class for your data
  . Autoclass
  . Set a default claass
- Standard- choose
  Nearline
  coldline
  Archive
- click on continue
- Choose how to control access to objects
  .Prevent public access
  denalbe enforce public prevention on this bucket
  .Access control
   Fine-grained
- Choose how to protect object data
- click on create
- now click on gcp-training-feb20 bucket
- click on upload
- you can see the authenticated url after uploading, you can try accessing

ACL Level
- if you click on edit access after clicking on object
- you can add entry, 
   entity - domain,user, project, publice
   name-
   access

- Edit defualt storage class
There are four basic cloud stroage  classeds
- Standard Storage- Hot data
- Nearline Storage- Once per month
- Coldline Storage- Once every 90 days
- Archieve Storage- Once a year


bucket level
- click on bucket
- click on 3 dots, click on edit access


GCS- Security
- IAM (Identity access Management) Policy- applies at bucket elvel
- ACL - Access control list object level 

GCS- Security versioning
-Applies at bucket and disabled by deffault
- if you enable versioning,intead of override, itset versions at file levlvevel
- while accessing object, it give live object
- version names start with #

Lify Cycle Policies
- day 0 save objects-- standard
- 6 months move to backp-- nearline
- 1 year  move to archive-- coldline
- 5 years deletge--delete file



GCS- Security versioning
- click on bucket
- click on protection
- you can see object oversioning( for data recover)
 you can enable  - object versioning 
- you will also have soft delete policy 
 (it prevent deletion of the file it is like a bind)

- in the bucket you can setup, Lify cycle rule
- click on add ule
- select an action- choose dlete object
- set conditions, age-2
  choose created before
  chooose number of newer versions
- click on continue
- click on create

once you open the cloud shell terminal
- if you see different project
- gcloud config set project Paperlive-GCP
- you will swithch to Paperlive-GCP Project 
- gsutil mb gs://gcp-command-demo

- gsutil ls (listing storage buckets)

- ls -l
- gsutil cp frontend.yml gs://gcp-command-demo


Compute Engine-Virtual Machine Instances
=================================================
- An instance is a virtual machine an compute engine which can create windows and os operating systems


Compute Engine -Features


Compute Engine- Acccess
- Linux: SSH
 - SSH from the google cloud console or cloud shell via the google clodu SDK
 - SSH from computer or third part client and generate key pair
 - Requires firewall rule to allow tcp:22

 Windows: RDP
 - RDP clients
 - Requires setting the windows password
 - Requires firewall rule to allow tcp: 3380

Compute Engine -Lab-1
--------------------------
-Create Linux VM instance
-Configure  web server
-Access from public network

- click on Demo-PaperLive project, click on compute engine
First time when you are using compute engine, You should engable  api
- click on enable
- click on VM instances, click on create VM
Machine Configuration
- Name- web-vm
- Region- us-central1 (lowa)
- zone- us-central1-a
- select general purpose
- choose e2
- In machine type, click the down arrwo button- you can choose standard/ shared core
 e2-medium( 2cpu 4 gb memory)
Os and storage
- click on change
boot disk
public images   custom imgaes snapshots archive snapshots

public images
- Operating system- 
- Version - CentOS stream 9
- Boot disk type- Balanced persistent disk
- Size GB- 2OGB
- click on select
Networking
- allow http traffic
Observavility
- default
Security
- default
Advanced
- default
- click on create

Once vm is created
- Now click on ssh
- click on authorize
-  df -h 
- sudo yum install httpd -y
- echo '<!doctype html><html><body><h1>Hello.. Welcome to
GCP</h1></body></html>' | sudo tee /var/www/html/index.html
- sudo bash
- systemctl restart httpd
- now take the external ip and and put in the browser
-


Unique Features
- Live Migration and Auto-restart
- Spot instances
- TPU

Preemptible/spot instances
- Create and run a mahcine much  lower than normal price
- compute engine can terminate anytime without informing you 
- pv cannot live migrate
- cannot create using free tier credits



 In advanced
 Provisioning model
- Vm proivisioning mode
 spot
- set a time limit for the VM
- Gracefullly shut down the vm

On Vm termination 
stop


Sole tenant nodes:

- Select sole-tenant nodes
- noe group properties
- node template properties
- configure auto scaling

Theory
- Sole tenant node is a physical compute engine serer that is deidicated projects Vms
- Use Sole-tenant nodes to keep your Vms physically separated from  Vm;s in other projects
- Vms running on sole-tenant nodes an use the same compute engine feautres  as other Vms
- suitable for workload like gaming which require hihger performance 
- Multiple Vms with various size can be provisioned


Storage Options:
====================
1. Persistent disk
 . standard
 . SSD
 . Balanced
 . Extreme
 . Hyper Extreme
Persistent disk
- Block storage, can attach from network storage
- functions like a physical disk in a laptop or server
- os disk is a persistent disk
- available in standard, SSD, Balanced and extreme variants
- Independent from virtual machine
- you can delete compute instance keeping your persistent  disk
- performance scale up with size
- supports only dynamic resizing
- automatic encruption
- scope is zonal, regional replication is available
- maximum up to 128 number can be addded in one indststance



Creating a VM

- click on Demo-PaperLive project, click on compute engine
First time when you are using compute engine, You should engable  api
- click on enable
- click on VM instances, click on create VM
Machine Configuration
- Name- web-vm
- Region- us-central1 (lowa)
- zone- us-central1-a
- select general purpose
- choose e2
- In machine type, click the down arrwo button- you can choose standard/ shared core
 e2-medium( 2cpu 4 gb memory)
Os and storage
- click on change
boot disk
public images   custom imgaes snapshots archive snapshots

public images
- Operating system- 
- Version - CentOS stream 9
- Boot disk type- Balanced persistent disk
- Size GB- 2OGB
- click on select
Networking
- allow http traffic
Observavility
- default
Security
- default
Advanced
- default
- click on create
- click on vm- click on ssh
- sudo apt install apache2 -y
- sudo systemctl start apache2
- echo '<!doctype html><html><body><h1>Hello.. Welcome to
GCP</h1></body></html>' | sudo tee /var/www/html/index.html
- sudo systemctl restart apache2
- 

Now in storage
- click on disks
- In  disk types( you can see  all disks)
- In compare disks( comparision between standard, balanced, ssd,extreme)
- size
- privoisoned IOPS
- disk source type- image
- disk setting
 disk type- Balanced persistent diks
 size -100
Encryption
- google-managed encryption key


Local SSD
- Local SSD are physically attached to the server  that host your VM insstance
- Local SSD have higher throuhput and lower latency than standard  persistent disks or SSD persistent disks
- The data that you store on a local SSD  persists only until 
- Fixed size 375 GB up to 9 TB can be attached
- VM with local SSD cannot live migrate
- not avaiable for shared core
- local SSD can attach only while Vm creation


While creating V

Creating a VM

- click on Demo-PaperLive project, click on compute engine
First time when you are using compute engine, You should engable  api
- click on enable
- click on VM instances, click on create VM
Machine Configuration
- Name- web-vm
- Region- us-central1 (lowa)
- zone- us-central1-a
- select general purpose
- choose n4
- In machine type, click the down arrwo button- you can choose standard/ shared core
 e2-medium( 2cpu 4 gb memory)
Os and storage
- click on add local ssd
- disk capacity - 375 GB
- click on change
boot disk
public images   custom imgaes snapshots archive snapshots

public images
- Operating system- 
- Version - CentOS stream 9
- Boot disk type- Balanced persistent disk
- Size GB- 2OGB
- click on select
Networking
- allow http traffic
Observavility
- default
Security
- default
Advanced
- default
- click on create
- click on vm- click on ssh
- sudo apt install apache2 -y
- sudo systemctl start apache2
- echo '<!doctype html><html><body><h1>Hello.. Welcome to
GCP</h1></body></html>' | sudo tee /var/www/html/index.html
- sudo systemctl restart apache2
-



Deleting the VM-- Compute Engine- lab-1
------------------------------
- frst edit the vm instance  deletion rule
  select keep the disk instead of delete the disk while deleting the VM
- Now delet the VM and create new vm in same zone

- click on VM instances, click on create VM
Machine Configuration
- Name- web-vm
- Region- us-central1 (lowa)
- zone- us-central1-a
- select general purpose
- choose n4
- In machine type, click the down arrwo button- you can choose standard/ shared core
 e2-medium( 2cpu 4 gb memory)
Os and storage

- click on change
boot disk
public images   customimgaes snapshots archivesnapshots existingdisks

existingdisks
disk- web-vm

Networking
- allow http traffic
Observavility
- default
Security
- default
Advanced
- default
- click on create
- click on vm- click on ssh
- sudo apt install apache2 -y
- sudo systemctl start apache2
- echo '<!doctype html><html><body><h1>Hello.. Welcome to
GCP</h1></body></html>' | sudo tee /var/www/html/index.html
- sudo systemctl restart apache2
-
- Now click on storage
- click on disks



Compute Engine lab-2
------------------------------
Delete the VM instance keeping disks and recreate
Create a new VM in different zone using current disk

above will fail it is not possible as it is scope is not available for shared core




Regional Persistent
--------------------
- Regional persistent disk is a storage option that provides
  synchronous replication of data between two zones in a region
- High availablity applications
- Redundancy during zonal failure
- Primary standby configuration
- failover to secondary if primary instance fails
- Heartbeat is responsible for failure detection

Snapthost:
------------

Compute Engine -lab-3
=========================
Create snapshot of disk
create snapshot scheudle
restore vm from snapshot
create machine image and restore from it


-Period back up solution of persistent disk
-snapshots are incremental and automatically compressed
-scope is global

-The first successful snapshot of a persisten disk is a full snapshot that 
contain data on the persistent disk.

-The second snapshot only contains any new data or modified data since the first snapshot
snapshot 3 contains any new or changed data since snapshot 2 but wont contain any unchnaged data from ssnapshot 1 or 2

- Regional and multi regional storage locations are avilable
- snapshots can be automated using snapshot schedules


-Regional and multi regional storage locations are available



- In storage, click on snapshots
- clik on create snapshots
- name- snapshot-od-disk
- snapshot source type- disk
- source disk- web-vm
- type- snapshot--chosse
        instant
        arhcive snapshot
- location 
  regional 
  select- asia-stouth1 (munaiu)
- click on create

- click on VM instances, click on create VM
Machine Configuration
- Name- web-vm
- Region- us-central1 (lowa)
- zone- us-central1-a
- select general purpose
- choose n4
- In machine type, click the down arrow button- you can choose standard/ shared core
 e2-medium( 2cpu 4 gb memory)
Os and storage
- click on change
- click on snapshot
- snapshot name- snaposhot-os disk -webvm

Automation of snapshot
----------------------
- click on snapshot
- click on create a snapshot schedule
- name - daily-backup
disk location
- schedule location
  region- us-centroal

storage location
 multi regional
 regional--- choose
 selecti location - us-central
 - schedule frequence- daily 
- start time
- auto delete - 30 days
Deletion rule
 After you delete the disk that uses this scheudle]
- keep snapshots
- click on create



Machine image:
-----------------------
Machine image contains a VM's properties, metadata , permissions and data from all itls
attached disks, You can use a machine image to create backup or resotore a VM

- create a machine image
- Name- mi-web
- Source Vm instance- web-vm-from-disk
locatino
 Multi-regional
 regional




From Cloud Shell terminal
============================
- gcloud compute isntances list
- gcloud compute disks list
- gcloud compute instances create vm1
- gcloud compute instances delete web-vm-from-disk



Load Balancers
=======================
 Load Balancing-types
- External load balancing- when your users reach your application from the internet
- Internal load balancing- when your clietns are insde of google cloud
- Regional load balancing- when your applications are available in single region
- Global load balancing for wehn your application are available across the worlds

                     Load Balancing
=-------------------------------------------------------=
      |                                          |
   External                                    Internal

|            |                                    |
global       regional                            regional
 |              |
http/https      network       
ssl proxy
tcp proxy


OSI Network Stack

User
application layer-- http/https
presentation layer
session layer--------SSL Proxy
transport layer------TCP Proxy
network layer -------Network
data link layer
physical layer


Global or Regional??
------------------------------
- Use global load balancing when your backend are distributed across  multiple regions
  your users need access to the same applications and content and you want to provide access by using a single anycast op address
- Global LB supports Ipv6
- Use regional load balancing when your backends are in one region
- Regional LB supports ipv4

External or Internal??
--------------------------
External-  https, ssl proxiy, tcp proxy, tcp/udp network
Internal- tcp/udp, Internal https
                                                    HealthCheck  Firewall
           Application load balancer
Traffice--> Forwarding rule--> Target HTTPS proxy--> URL map--> bakcend service--> Backend         
                                    |
                                 ssl certificate

Backends:
--------------

- Instance groups
- Zonal network Endpoint group- One more internal ip address
- Internet Network endpoints groups- 
  Outside of google cloud hostname:port IP:port
- Serverless NEGs- App engine, Clou rule, Cloud function
- GCS Buckets


Autoscaling:
---------------
- Compute Engine offers autoscaling to automatically add ore remove VM instances
  from an instance group based on increase or decreases in lod
- Autoscaling let your apps gracefully handle your increaisng traffic
- it reduce cost whne lower number of resources required

Managed Instance groups:
-----------------------------
- it let you operate on multiple identical VM
- Supports autoscaling and load balancing
- hihg availablity -helps you to keep VM running
- health checkup and auto healing
- regional coverage, protect app from zonal failure
- automatic scalablity base don predefined config
- automatic updating( rolling update)

Un managed instance groups:

Load Balancing lab:
--------------------------

Creating instance template:
---------------------------
- click on Paperlive-GCP project
- click on compute engine
- click on instance templates
- name- lb-template
- location - regional 
             global----- chose
- Machine Configuration
   - e2
- Machine type
   e2-micro (2 CPU,1 COR ,1 GBMemory)
- Provisioning model
  standard
- Boot disk
   default
- Identify and API access
  defualt  allow default acess
- Firewall
  allow http traffic
- In advanced
  - In Management
    Automation option
    #!/bin/bahs
    apt-get update
    <html><body><h1>Load Balancer lab </h1>
    <p>Server Name : $(hostname)</p>
    </body></html>
- click on create

Creating Instance groups
-----------------------------
- click on Paperlive-GCP project
- click on compute engine
- click on instance groups
In New Managed instance group(stateless)
- Name- ib-group-us
- Instance template- select lb-template
- location
   single zone
   multiple zone--- Choose
  region          zones
  target distribution shape
  even ---------------------choose
   (Distribute managed instances evenly across zones)
  balanced
   (Distribute managed instances as evenly as possible across zones give
    availablity of resources in each zone)
  any
   ( Deploy managed instances to one or multiple zones based on availablity of resources
     and reservations)
  any single zone
    (Deploy managed instances within a single zone)
- Autoscale
  Auto-scaling mode
  On: add and remoe instances to and from the group
  scaleout: only add instances to the group
  off: do not auto scale

  Minimum number of instances     Maximum number of instances 
  3                               5

- Edit signal
  signal type
   -CPU utilisation
  Target CPU Utilisation
   -90
-Auto scaling schedules
 -Inititalisation period
   120 seconds
- Auto healing
  -health check
   lb-health (http)
  - initial delay
    300 
   clickon create a health check
    - Name- lb-health
    -scope- global
    -region-us-centrail1
    - protocal -http, port-80
    - Proxy protocol None
    - Request path* - /
    - health check
- Health criteria
   check interval-30 sec
   timeout-5  sec
   healthy threshold-2       consecutive sucesss
   Unhealth threshold-5      consecutive failures


Create Load Balancing
---------------------------
- In Network services
- click on load balancing
- click on create load balancer
- type of load balancer
  application load balancer(http/https)  --choose
  networkload load balancer( tcp/udp/ssl)
- clikc on next
- public facing or internal
  publick facing(external)- choose
  internal facing(internal)
- click on next
- global or single region deployment
  - best for global worlloads 
  - best for regional workloads- choose
- click on next
- click on configure
- load balancer name- lb-demo
- region -us-centrail
- network- defualt
  click on reserve subnet
  name- lb-proxy
  region- us-centrail
  network defualt
  ipv4 range - 10.90.0.0/25
  click on add
- Front-end configuration
   Name- lb-gron
   protocol http
   network service tier- premium--choose
                         standard
   port-80
   ip address 
    -ephemeral (temporoar) - choose
    - click on create ip addresses
    - name -lb-ip
    - click on reserve
- backend configuration
   click on create
  - name- lb-backend
  - backend type- instance group
  - protocol - http
  - name port - http
  - timeout- 30
  - ip adddress selection policy
    ip stack type
    - ipv4
    - instance group
       lb-group-us
    -port no- 80
    -balancing mode
     -utitilisation
    - maximum backend utilisation
      80
    - Health check
      click on create a health check 
      - name- lb-health-ext
      -region-us-centrail1
      - protocal -http, port-80
      - Proxy protocol None
      - Request path* - /
      - health check
      - Health criteria
        check interval-30 sec
        timeout-5  sec
        healthy threshold-2       consecutive sucesss
        Unhealth threshold-3
      - click on save
- now load balancer is ready access it with  ip address

   

Command to increase the utilisation of CPU
- yes >/dev/null &
- then test with top command







