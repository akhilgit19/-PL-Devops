
 
#########################
Day 20 : 11th August 2025
#########################
		
		
	Container Orchestration using Kubernetes ::::
	
	- Kubernetes!
	
	Docker Container : 
	
	AWS :	ECS/ECR/EKS
	
	Azure : ACS/ACR/AKS
	
	GCP :	GCE/GCR/GKE

	- Kubernetes!
	
		- It is a Open-Source Container Orchestration Tool 
		- Kubernetes is used to Deploy any type of Containers.
		- It is used to ensure high availability of the Applications/services running thru Containers.
		- Used to Ensure High Availability of Containers by creating Replicas of Containers.
		- It supports Auto-Scaling & Load Balancing.		
		


#########################
Day 21 : 12th August 2025
#########################	
	
	- Kubernetes Architecture :::
		
		Based on:
		
			Kubernetes_Control_Plane/Master 
				- Kubernetes_WorkerNode1,2,3,4,5	



		- App Config file - Manifest file - *.yaml / *.json		
        - Once ready with the manifest file , we use kubectl to interact with Kubernetes master components
        - API server acts as a interface betwween user request and kubernetes master
        - Any task will execute in POD
        - Scheduler will always idenfitier the suitable node
        - kublet is an agent upon running  kubernetes worker components
        - CRI- Container runtime interface, it will download the image then kublet will identify and deploy it into the pod
        - Controll Manger ensures that pod is running in it;s own desire state and maintain replicass and self healing of the pods
        - ETCD  maintains information about kuberntes cluster, master worker nodes , pods


	- Kubernetes!
	
		- It is a Open-Source Container Orchestration Tool 
		- Kubernetes is used to Deploy any type of Containers.
		- It is used to ensure high availability of the Applications/services running thru Containers.
		- Used to Ensure High Availability of Containers by creating Replicas of Containers.
		- It supports Auto-Scaling & Load Balancing.
		- Self-Healing
		
		- App Config file - Manifest file - *.yaml / *.json		

		
	- Kubernetes Architecture Components :::	
	
		- API Server 			# It act as an interface between the User and Kubernetes Master
		
		- ETCD					# Is a single point of source for Kubernetes
		
		- Scheduler				# Is used to identify the healthy worknode for the pods deployment 
		
		- Controller Manager	# To ensure the pods are running in its desired state.
								# Perform Self-Healing 
								# Ensure High Availability 
		
		
		- Kubelet				# Is a Kubernetes Agent, that actually deploy the pods 

		- CRI - Container Runtime Interface (Container-D)
								# Is to connect to Container Registry and download the Container Images 
								
		- Kube-Proxy 			# To enable Pod Networking.		
								# Assign pod IP Address based on the Network Plugins 
	
	

	- Terminologies & Kubernetes Concepts :::
	
	
		- Kubernetes_Cluster	# Is a collection of Kubernetes Worker Nodes 

		- Non-Prod Environments :::					=====>				Production Environments :::						
			
			Dev 
			
			Build 
			
			Test :
				QA 
				
				UAT 							==============>				Production Servers1,2,3,4,5
		
		

		
			- Kubernetes_Master :
			
					- Kubernetes_WorkerNode1
					- Kubernetes_WorkerNode2
					- Kubernetes_WorkerNode3
					- Kubernetes_WorkerNode4
					- Kubernetes_WorkerNode5
				
		- Pods 					# Atomic Unit of Schedule
								# Used to Execute Micro-Services 
								
		- Kubectl				# Is a command line utility to interact with Kubernetes Master 
		
		- Kubernetes Objects :
		
			- Pods 					# Atomic Unit of Schedule
									# Used to Execute Micro-Services 		
									
			- Deployment Controller Objects :::
			
				- Replicaset				
				- Deployment 
				
			- Kubernetes Services :::
			
				- Cluster-IP Service 
				- NodePort Services 
				- Load Balancer 
				
			- Kubernetes Volumes ::
			
				- EmptyDir 
				- Hostpath Volume 
				- Persistant Volume 
				- Persistant Volume Claim
				
				
	
	- Working with Kubernetes
	
		- Install and Configure Kubernetes Cluster.

			- Kubernetes_Control_Plane/Master (VM)
			
				- Kubernetes_WorkerNode1 (VM)
			
				- Kubernetes_WorkerNode2 (VM)	
				

	Open-Source Kubernetes :::
	
		https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/
		
		
		Kubernetes_Master			(VM)
			Kubernetes_WorkNode1	(VM)
			Kubernetes_WorkNode2	(VM)
			
			
		Minikube 		--> 	1 Node Configuration
			VM -> 
			
		Kubeadm -->
		
		
	Installation of Kubernetes using Kubeadm :::	
	
		1. Launch 3 VMs on AWS Cloud (Ubuntu v22.04) --> (1 Master Node, 2 WorkerNodes)
		
		In all the Nodes(i.e., Master Node and WorkerNodes):
		
			2. Allow all traffic for all the nodes - just for this demo
			3. Change the HostName of all the Nodes
			4. Disable swap configuration in all the nodes
			5. Install Docker in all the nodes
			6. Install CRI - 'Container-D' in all the nodes
			7. Install Kubeadm,kubelet,kubectl 
			8. Enable Kubelet	
		   
		Only on Master Node:
		
			9. Execute Kubeadm Init Command 		# To initialize Kubernetes Master Node
			10. Enable user Access to Kubernetes
			11. Install flannel Network plugins for kubeproxy

		Only on WorkerNodes:		
		
			12. Execute Kubeadm Join Command 		# To attach the Worknodes with Kubernetes Master Node.



############################################################################################################

#Kube-Master(Controller)  - VM 			# Ubuntu v22.04 - 
#	Kube-Worker1
#	Kube-Worker2 

#https://kubernetes.io/docs/setup/
#https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
	
#Add Port: 0 - 65535

#Allow All Traffic for Demo!

#Master Nodes :
####  6443, 2379-2380, 10250, 10259, 10257

Worker Nodes :
####  10250, 30000-32767(NodePort Range)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###On Both Master and Worker Nodes:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

sudo -i

apt update -y

#Set the appropriate hostname for each machine.
#-----------------------------------------------
sudo hostnamectl set-hostname "kmaster-node"
exec bash

reboot

sudo hostnamectl set-hostname "worker-node1"
exec bash

reboot

sudo hostnamectl set-hostname "worker-node2"
exec bash

reboot


#To allow kubelet to work properly, we need to disable swap on both machines.

sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

#Install Docker :

apt install docker.io -y

#Step 1. Install containerd			Is a CRI 
#To install containerd, follow these steps on both VMs:

#Load the br_netfilter module required for networking.

sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

#To allow iptables to see bridged traffic, as required by Kubernetes, we need to set the values of certain fields to 1.

sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

#Apply the new settings without restarting.

sudo sysctl --system

#Install curl.



sudo apt install curl -y	

#Get the apt-key and then add the repository from which we will install containerd.

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

#Update and then install the containerd package.

sudo apt update -y 
sudo apt install -y containerd.io

#Set up the default configuration file.

sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml

#Next up, we need to modify the containerd configuration file and ensure that the cgroupDriver is set to systemd. To do so, #edit the following file:

sudo vi /etc/containerd/config.toml

#Scroll down to the following section:

#[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
#And ensure that value of SystemdCgroup is set to true Make sure the contents of your section match the following:

#[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
#    BinaryName = ""
#    CriuImagePath = ""
#    CriuPath = ""
#    CriuWorkPath = ""
#    IoGid = 0
#    IoUid = 0
#    NoNewKeyring = false
#    NoPivotRoot = false
#    Root = ""
#    ShimCgroup = ""
##    SystemdCgroup = true

#Finally, to apply these changes, we need to restart containerd.

sudo systemctl restart containerd
sudo systemctl status containerd


#Step 2. Install Kubernetes
#With our container runtime installed and configured, we are ready to install Kubernetes.

#Add the repository key and the repository.

sudo apt-get update

# apt-transport-https may be a dummy package; if so, you can skip that package

sudo apt-get install -y apt-transport-https ca-certificates curl gpg


# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
# sudo mkdir -p -m 755 /etc/apt/keyrings
#mkdir /etc/apt/keyrings

sudo mkdir -p -m 755 /etc/apt/keyrings

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl

#Finally, enable the kubelet service on both systems so we can start it.

sudo systemctl enable kubelet

#######################################################################################################



#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###On Master Node:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Step 3. Setting up the cluster
#With our container runtime and Kubernetes modules installed, we are ready to initialize our Kubernetes cluster.

#Run the following command on the master node to allow Kubernetes to fetch the required images before cluster initialization:

sudo kubeadm config images pull

#Initialize the cluster

#sudo kubeadm init --pod-network-cidr=10.244.0.0/16

sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=NumCPU --ignore-preflight-errors=Mem

#The initialization may take a few moments to finish. Expect an output similar to the following:

#Your Kubernetes control-plane has initialized successfully!
#To start using your cluster, you need to run the following as a regular user:

#root : visudo 

#su - devopsadmin 

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#Alternatively, if you are the root user, you can run:

#export KUBECONFIG=/etc/kubernetes/admin.conf

#You should now deploy a pod network to the cluster. Run kubectl apply -f [podnetwork].yaml with one of the options listed at Kubernetes.

#Then you can join any number of worker nodes by running the following on each as root:

#kubeadm join 102.130.122.60:6443 --token s3v1c6.dgufsxikpbn9kflf \
        --discovery-token-ca-cert-hash sha256:b8c63b1aba43ba228c9eb63133df81632a07dc780a92ae2bc5ae101ada623e00

#You will see a kubeadm join at the end of the output. Copy and save it in some file. We will have to run this command on the worker node to allow it to join the cluster. If you forget to save it, or misplace it, you can also regenerate it using this command:

#sudo kubeadm token create --print-join-command

#Now create a folder to house the Kubernetes configuration in the home directory. We also need to set up the required permissions for the directory, and export the KUBECONFIG variable.
#mkdir -p $HOME/.kube
#sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
#sudo chown $(id -u):$(id -g) $HOME/.kube/config
#export KUBECONFIG=/etc/kubernetes/admin.conf

#Deploy a pod network to our cluster. This is required to interconnect the different Kubernetes components.

kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml


#Expect an output like this:

#podsecuritypolicy.policy/psp.flannel.unprivileged created
#clusterrole.rbac.authorization.k8s.io/flannel created
#clusterrolebinding.rbac.authorization.k8s.io/flannel created
#serviceaccount/flannel created
#configmap/kube-flannel-cfg created
#daemonset.apps/kube-flannel-ds created

#Use the get nodes command to verify that our master node is ready.

kubectl get nodes

#Also check whether all the default pods are running:

kubectl get pods --all-namespaces

#You should get an output like this:

NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE
kube-system   coredns-6d4b75cb6d-dxhvf              1/1     Running   0          10m
kube-system   coredns-6d4b75cb6d-nkmj4              1/1     Running   0          10m
kube-system   etcd-master-node                      1/1     Running   0          11m
kube-system   kube-apiserver-master-node            1/1     Running   0          11m
kube-system   kube-controller-manager-master-node   1/1     Running   0          11m
kube-system   kube-flannel-ds-jxbvx                 1/1     Running   0          6m35s
kube-system   kube-proxy-mhfqh                      1/1     Running   0          10m
kube-system   kube-scheduler-master-node            1/1     Running   0          11m

#We are now ready to move to the worker node. Execute the kubeadm join from step 2 on the worker node. You should see an output similar to the following:

#This node has joined the cluster:
#* Certificate signing request was sent to apiserver and a response was received.
#* The Kubelet was informed of the new secure connection details.
#Run kubectl get nodes on the control-plane to see this node join the cluster.

#If you go back to the master node, you should now be able to see the worker node in the output of the get nodes command.

kubectl get nodes

#And the output should look as follows:

NAME          STATUS   ROLES                  AGE     VERSION
master-node   Ready    control-plane,master   18m40s   v1.24.2
worker-node   Ready    <none>                 3m2s     v1.24.2

#Finally, to set the proper role for the worker node, run this command on the master node:

#kubectl label node worker-node node-role.kubernetes.io/worker=worker

#To verify that the role was set:

kubectl get nodes

#The output should show the worker node’s role as follows:

NAME          STATUS   ROLES                  AGE     VERSION
master-node   Ready    control-plane,master   5m12s   v1.24.1
worker-node   Ready    worker                 2m55s   v1.24.1


sudo kubeadm token create --print-join-command 	# To generate New Token along with Kubeadm Join Command.
												# Run this command on the master node

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###Only in Worker Nodes:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

kubeadm join 172.31.38.168:6443 --token t4ujfp.nk1kr8f2o04nxkrk \
        --discovery-token-ca-cert-hash sha256:411ddb8859ce7686446e559623e9d8e8123f2db52befa6500838289e567681a5





			
Next :::
				
	- Working with Kubernetes Objects :::



#########################
Day 22 : 14th August 2025
#########################	



	- Working with Kubernetes Objects
	
		- Kubernetes Objects :
		
			- Pods 					# Atomic Unit of Schedule
									# Used to Execute Micro-Services 		
									
			- Deployment Controller Objects :::
			
				- Replicaset				
				- Deployment 
				
			- Kubernetes Services :::
			
				- Cluster-IP Service 
				- NodePort Services 
				- Load Balancer 
				
			- Kubernetes Volumes ::
			
				- EmptyDir 
				- Hostpath Volume 
				- Persistant Volume 
				- Persistant Volume Claim
				
			- Namespace!
			
				
	- Namespace!
	
		- Logical Partitioning of Kubernetes Cluster
		- Namespaces can be created based on Environments, Application Team, Services/Objects 
				
	
		Non-Prod :	-> Dev/QA/UAT 
		
		Kubernetes_Master			(VM)
			Kubernetes_WorkNode1	(VM)
			Kubernetes_WorkNode2	(VM)

	
		Prod :
		
		Kubernetes_Master			(VM)
			Kubernetes_WorkNode1	(VM)
			Kubernetes_WorkNode2	(VM)
			
-  Login to kmaster node
- sudo -i
- kubectl get nodes( You should be able to get list of nodest)
NAME             STATUS         ROLES        AGE        VERSION
kmaster-node     Ready         control-plane   46h       v1.29.15
worker-node1     Ready          <none>         46h       v1.29.15
worker-node2     Ready          <none>         46h       v1.29.15
- kubectl get pods
- kubectl get pods -- all-namespaces (You will see all the name spaces s in the cluster)
- kubectl get ns 
- kubectl get pods -- all-namespaces -o wide


*******************************************************************
# 1. 	Create Pod Object Manifest File :

# nginx-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
    tier: dev
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80


*******************************************************************


	- Pods :
	
	
		NodePort Service Create Port between - 30000 to 32767
				
		Roles :
		
			Kubernetes Developers 
			
			Kubernetes Administrators 
			
			Kubernetes Security Administrators 
			
			
			WorkerNode1,2,3,4,5,6,7,8,9,10 
			
				--> 1,3,5,7,9 ==> as Target group 
				
				--> 1 	==> Target Node 

	- Deployment Controller Objects :::
	
		- Replicaset				
		- Deployment 
		
	Controller Object :::
		ReplicaSet 
		Deployment 


	ReplicaSet :::
	
		--> Replicaset is used to execute the specific no. of pods in the cluster.
		--> Replicaset uses the Set Based Operator
		--> Used to replicate the pods and able to scale up/down
		--> The Replicasets will be automatically created, while creating Deployment Controller Object.
	
	Deployment Controller Object :::
	
		--> It is used to deploy the pods and ensure high availability of pods by creating pod replicas 
		--> 1. Create Muliple instance/replicas/copies of pods 
			2. Used to Scale-Up / Scale-Down the Pods 
			3. Used to Upgrade the application pods 
			4. Used to Down-grade/roll-back the application pods
		--> The upgrade/down-grade of application pods can be done without any downtime. 
		--> To achieve zero-downtime during upgrade/down-grade, By Default, it used Rolling-Update Deployment Strategy.
		

	App_snapshot_v1.0		--->	App_snapshot_img:v1.0	==> Published to Dockerhub.
	App_snapshot_v1.1		--->	App_snapshot_img:v1.1	==> Published to Dockerhub.	
	App_snapshot_v1.2		--->	App_snapshot_img:v1.2	==> Published to Dockerhub.	
	
	
	App_snapshot_v1.0		--->	App_snapshot_img:v1.0	==> Published to Dockerhub.
	
	
	Pod1 ==> App_snapshot_img:v1.0	Running!
	
	
	Deploy the Pods using Deployment Controller Object :
	
	
		- Create Deployment Object

		- Replicaset 					Replica = 3.
		
		- Pods Instances 				To Create 3 replicas of pod
		
	
	
	Pod1	=	App_snapshot_img:v1.0
	
	Pod2 	=	App_snapshot_img:v1.0
	
	Pod3 	=	App_snapshot_img:v1.0
	
	
	
	Upgrade Application :		using Rolling-Update Deployment Strategy.
	
	
		App_snapshot_v1.0		--->	App_snapshot_img:v1.1	==> Published to Dockerhub.	
		
		
		
	Pod1	=	App_snapshot_img:v1.0						====>			App_snapshot_img:v1.1	
	
	Pod2 	=	App_snapshot_img:v1.0						====>			App_snapshot_img:v1.1
	
	Pod3 	=	App_snapshot_img:v1.0						====>			App_snapshot_img:v1.1
	
	
	Scale-Up/Scale-Down :::
	
		- Increase/Decrease the Replicas of Pods 



		Web_Application :::
		
			--> Replica = 100 		==> 	1000 Users/Requests 			

				Replica	= 500		===> 	5000 Users/Requests		


			online Reservation System 
			
			amazon.in 
			
			
			Pod Replicas  			>		Taken care by Kubernetes Deployment Controller Object 			
			
			Node Replicas 			>		Add More nodes to the cluster (Horizontal Scaling)
			
			Upgrade the Existing Nodes 	
									> 	Eg.: t2.micro to t2.medium -> Update the Node - Vertical Scaling 			
		
		Kubernetes_Master			(VM)
			Kubernetes_WorkNode1	(VM)
			Kubernetes_WorkNode2	(VM)


*******************************************************************



2. Create and display Pods

# Create and display PODs

#kubectl apply -f nginx-pod.yaml

kubectl create -f nginx-pod.yaml
kubectl get pod
kubectl get pod -o wide
kubectl get pod nginx-pod -o yaml
kubectl describe pod nginx-pod


*******************************************************************

3. Test & Delete

# To get inside the pod
kubectl exec -it nginx-pod bash

# Create test HTML page
cat <<EOF > /usr/share/nginx/html/test.html
<!DOCTYPE html>
<html>
<head>
<title>Testing..</title>
</head>
<body>
<h1 style="color:rgb(90,70,250);">Hello, Everyone...!</h1>
<h2>Welcome to Kubernetes Demo :-) </h2>
</body>
</html>
EOF
exit

# Expose PODS using NodePort service

kubectl expose pod nginx-pod --type=NodePort --port=80

#NodePort Service Create Port between - 30000 to 32767

kubectl get svc

# Node-Port Range : 30000 - 32767 

# Display Service and find NodePort
kubectl describe svc nginx-pod

# Open Web-browser and access webapge using 
http://<external-nodeip>:<nodeport>/test.html

#eg.: http://43.205.215.212:32049



# Delete pod & svc
kubectl delete svc nginx-pod
kubectl delete pod nginx-pod



