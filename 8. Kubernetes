
 
#########################
Day 20 : 11th August 2025
#########################
		
		
	Container Orchestration using Kubernetes ::::
	
	- Kubernetes!
	
	Docker Container : 
	
	AWS :	ECS/ECR/EKS
	
	Azure : ACS/ACR/AKS
	
	GCP :	GCE/GCR/GKE

	- Kubernetes!
	
		- It is a Open-Source Container Orchestration Tool 
		- Kubernetes is used to Deploy any type of Containers.
		- It is used to ensure high availability of the Applications/services running thru Containers.
		- Used to Ensure High Availability of Containers by creating Replicas of Containers.
		- It supports Auto-Scaling & Load Balancing.		
		


#########################
Day 21 : 12th August 2025
#########################	
	
	- Kubernetes Architecture :::
		
		Based on:
		
			Kubernetes_Control_Plane/Master 
				- Kubernetes_WorkerNode1,2,3,4,5	



		- App Config file - Manifest file - *.yaml / *.json		
        - Once ready with the manifest file , we use kubectl to interact with Kubernetes master components
        - API server acts as a interface betwween user request and kubernetes master
        - Any task will execute in POD
        - Scheduler will always idenfitier the suitable node
        - kublet is an agent upon running  kubernetes worker components
        - CRI- Container runtime interface, it will download the image then kublet will identify and deploy it into the pod
        - Controll Manger ensures that pod is running in it;s own desire state and maintain replicass and self healing of the pods
        - ETCD  maintains information about kuberntes cluster, master worker nodes , pods


	- Kubernetes!
	
		- It is a Open-Source Container Orchestration Tool 
		- Kubernetes is used to Deploy any type of Containers.
		- It is used to ensure high availability of the Applications/services running thru Containers.
		- Used to Ensure High Availability of Containers by creating Replicas of Containers.
		- It supports Auto-Scaling & Load Balancing.
		- Self-Healing
		
		- App Config file - Manifest file - *.yaml / *.json		

		
	- Kubernetes Architecture Components :::	
	
		- API Server 			# It act as an interface between the User and Kubernetes Master
		
		- ETCD					# Is a single point of source for Kubernetes
		
		- Scheduler				# Is used to identify the healthy worknode for the pods deployment 
		
		- Controller Manager	# To ensure the pods are running in its desired state.
								# Perform Self-Healing 
								# Ensure High Availability 
		
		
		- Kubelet				# Is a Kubernetes Agent, that actually deploy the pods 

		- CRI - Container Runtime Interface (Container-D)
								# Is to connect to Container Registry and download the Container Images 
								
		- Kube-Proxy 			# To enable Pod Networking.		
								# Assign pod IP Address based on the Network Plugins 
	
	

	- Terminologies & Kubernetes Concepts :::
	
	
		- Kubernetes_Cluster	# Is a collection of Kubernetes Worker Nodes 

		- Non-Prod Environments :::					=====>				Production Environments :::						
			
			Dev 
			
			Build 
			
			Test :
				QA 
				
				UAT 							==============>				Production Servers1,2,3,4,5
		
		

		
			- Kubernetes_Master :
			
					- Kubernetes_WorkerNode1
					- Kubernetes_WorkerNode2
					- Kubernetes_WorkerNode3
					- Kubernetes_WorkerNode4
					- Kubernetes_WorkerNode5
				
		- Pods 					# Atomic Unit of Schedule
								# Used to Execute Micro-Services 
								
		- Kubectl				# Is a command line utility to interact with Kubernetes Master 
		
		- Kubernetes Objects :
		
			- Pods 					# Atomic Unit of Schedule
									# Used to Execute Micro-Services 		
									
			- Deployment Controller Objects :::
			
				- Replicaset				
				- Deployment 
				
			- Kubernetes Services :::
			
				- Cluster-IP Service 
				- NodePort Services 
				- Load Balancer 
				
			- Kubernetes Volumes ::
			
				- EmptyDir 
				- Hostpath Volume 
				- Persistant Volume 
				- Persistant Volume Claim
				
				
	
	- Working with Kubernetes
	
		- Install and Configure Kubernetes Cluster.

			- Kubernetes_Control_Plane/Master (VM)
			
				- Kubernetes_WorkerNode1 (VM)
			
				- Kubernetes_WorkerNode2 (VM)	
				

	Open-Source Kubernetes :::
	
		https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/
		
		
		Kubernetes_Master			(VM)
			Kubernetes_WorkNode1	(VM)
			Kubernetes_WorkNode2	(VM)
			
			
		Minikube 		--> 	1 Node Configuration
			VM -> 
			
		Kubeadm -->
		
		
	Installation of Kubernetes using Kubeadm :::	
	
		1. Launch 3 VMs on AWS Cloud (Ubuntu v22.04) --> (1 Master Node, 2 WorkerNodes)
		
		In all the Nodes(i.e., Master Node and WorkerNodes):
		
			2. Allow all traffic for all the nodes - just for this demo
			3. Change the HostName of all the Nodes
			4. Disable swap configuration in all the nodes
			5. Install Docker in all the nodes
			6. Install CRI - 'Container-D' in all the nodes
			7. Install Kubeadm,kubelet,kubectl 
			8. Enable Kubelet	
		   
		Only on Master Node:
		
			9. Execute Kubeadm Init Command 		# To initialize Kubernetes Master Node
			10. Enable user Access to Kubernetes
			11. Install flannel Network plugins for kubeproxy

		Only on WorkerNodes:		
		
			12. Execute Kubeadm Join Command 		# To attach the Worknodes with Kubernetes Master Node.



############################################################################################################

#Kube-Master(Controller)  - VM 			# Ubuntu v22.04 - 
#	Kube-Worker1
#	Kube-Worker2 

#https://kubernetes.io/docs/setup/
#https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
	
#Add Port: 0 - 65535

#Allow All Traffic for Demo!

#Master Nodes :
####  6443, 2379-2380, 10250, 10259, 10257

Worker Nodes :
####  10250, 30000-32767(NodePort Range)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###On Both Master and Worker Nodes:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

sudo -i

apt update -y

#Set the appropriate hostname for each machine.
#-----------------------------------------------
sudo hostnamectl set-hostname "kmaster-node"
exec bash

reboot

sudo hostnamectl set-hostname "worker-node1"
exec bash

reboot

sudo hostnamectl set-hostname "worker-node2"
exec bash

reboot


#To allow kubelet to work properly, we need to disable swap on both machines.

sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

#Install Docker :

apt install docker.io -y

#Step 1. Install containerd			Is a CRI 
#To install containerd, follow these steps on both VMs:

#Load the br_netfilter module required for networking.

sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

#To allow iptables to see bridged traffic, as required by Kubernetes, we need to set the values of certain fields to 1.

sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

#Apply the new settings without restarting.

sudo sysctl --system

#Install curl.



sudo apt install curl -y	

#Get the apt-key and then add the repository from which we will install containerd.

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

#Update and then install the containerd package.

sudo apt update -y 
sudo apt install -y containerd.io

#Set up the default configuration file.

sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml

#Next up, we need to modify the containerd configuration file and ensure that the cgroupDriver is set to systemd. To do so, #edit the following file:

sudo vi /etc/containerd/config.toml

#Scroll down to the following section:

#[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
#And ensure that value of SystemdCgroup is set to true Make sure the contents of your section match the following:

#[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
#    BinaryName = ""
#    CriuImagePath = ""
#    CriuPath = ""
#    CriuWorkPath = ""
#    IoGid = 0
#    IoUid = 0
#    NoNewKeyring = false
#    NoPivotRoot = false
#    Root = ""
#    ShimCgroup = ""
##    SystemdCgroup = true

#Finally, to apply these changes, we need to restart containerd.

sudo systemctl restart containerd
sudo systemctl status containerd


#Step 2. Install Kubernetes
#With our container runtime installed and configured, we are ready to install Kubernetes.

#Add the repository key and the repository.

sudo apt-get update

# apt-transport-https may be a dummy package; if so, you can skip that package

sudo apt-get install -y apt-transport-https ca-certificates curl gpg


# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
# sudo mkdir -p -m 755 /etc/apt/keyrings
#mkdir /etc/apt/keyrings

sudo mkdir -p -m 755 /etc/apt/keyrings

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl

#Finally, enable the kubelet service on both systems so we can start it.

sudo systemctl enable kubelet

#######################################################################################################



#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###On Master Node:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Step 3. Setting up the cluster
#With our container runtime and Kubernetes modules installed, we are ready to initialize our Kubernetes cluster.

#Run the following command on the master node to allow Kubernetes to fetch the required images before cluster initialization:

sudo kubeadm config images pull

#Initialize the cluster

#sudo kubeadm init --pod-network-cidr=10.244.0.0/16

sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=NumCPU --ignore-preflight-errors=Mem

#The initialization may take a few moments to finish. Expect an output similar to the following:

#Your Kubernetes control-plane has initialized successfully!
#To start using your cluster, you need to run the following as a regular user:

#root : visudo 

#su - devopsadmin 

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#Alternatively, if you are the root user, you can run:

#export KUBECONFIG=/etc/kubernetes/admin.conf

#You should now deploy a pod network to the cluster. Run kubectl apply -f [podnetwork].yaml with one of the options listed at Kubernetes.

#Then you can join any number of worker nodes by running the following on each as root:

#kubeadm join 102.130.122.60:6443 --token s3v1c6.dgufsxikpbn9kflf \
        --discovery-token-ca-cert-hash sha256:b8c63b1aba43ba228c9eb63133df81632a07dc780a92ae2bc5ae101ada623e00

#You will see a kubeadm join at the end of the output. Copy and save it in some file. We will have to run this command on the worker node to allow it to join the cluster. If you forget to save it, or misplace it, you can also regenerate it using this command:

#sudo kubeadm token create --print-join-command

#Now create a folder to house the Kubernetes configuration in the home directory. We also need to set up the required permissions for the directory, and export the KUBECONFIG variable.
#mkdir -p $HOME/.kube
#sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
#sudo chown $(id -u):$(id -g) $HOME/.kube/config
#export KUBECONFIG=/etc/kubernetes/admin.conf

#Deploy a pod network to our cluster. This is required to interconnect the different Kubernetes components.

kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml


#Expect an output like this:

#podsecuritypolicy.policy/psp.flannel.unprivileged created
#clusterrole.rbac.authorization.k8s.io/flannel created
#clusterrolebinding.rbac.authorization.k8s.io/flannel created
#serviceaccount/flannel created
#configmap/kube-flannel-cfg created
#daemonset.apps/kube-flannel-ds created

#Use the get nodes command to verify that our master node is ready.

kubectl get nodes

#Also check whether all the default pods are running:

kubectl get pods --all-namespaces

#You should get an output like this:

NAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE
kube-system   coredns-6d4b75cb6d-dxhvf              1/1     Running   0          10m
kube-system   coredns-6d4b75cb6d-nkmj4              1/1     Running   0          10m
kube-system   etcd-master-node                      1/1     Running   0          11m
kube-system   kube-apiserver-master-node            1/1     Running   0          11m
kube-system   kube-controller-manager-master-node   1/1     Running   0          11m
kube-system   kube-flannel-ds-jxbvx                 1/1     Running   0          6m35s
kube-system   kube-proxy-mhfqh                      1/1     Running   0          10m
kube-system   kube-scheduler-master-node            1/1     Running   0          11m

#We are now ready to move to the worker node. Execute the kubeadm join from step 2 on the worker node. You should see an output similar to the following:

#This node has joined the cluster:
#* Certificate signing request was sent to apiserver and a response was received.
#* The Kubelet was informed of the new secure connection details.
#Run kubectl get nodes on the control-plane to see this node join the cluster.

#If you go back to the master node, you should now be able to see the worker node in the output of the get nodes command.

kubectl get nodes

#And the output should look as follows:

NAME          STATUS   ROLES                  AGE     VERSION
master-node   Ready    control-plane,master   18m40s   v1.24.2
worker-node   Ready    <none>                 3m2s     v1.24.2

#Finally, to set the proper role for the worker node, run this command on the master node:

#kubectl label node worker-node node-role.kubernetes.io/worker=worker

#To verify that the role was set:

kubectl get nodes

#The output should show the worker node’s role as follows:

NAME          STATUS   ROLES                  AGE     VERSION
master-node   Ready    control-plane,master   5m12s   v1.24.1
worker-node   Ready    worker                 2m55s   v1.24.1


sudo kubeadm token create --print-join-command 	# To generate New Token along with Kubeadm Join Command.
												# Run this command on the master node

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###Only in Worker Nodes:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

kubeadm join 172.31.38.168:6443 --token t4ujfp.nk1kr8f2o04nxkrk \
        --discovery-token-ca-cert-hash sha256:411ddb8859ce7686446e559623e9d8e8123f2db52befa6500838289e567681a5





			
Next :::
				
	- Working with Kubernetes Objects :::



#########################
Day 22 : 14th August 2025
#########################	



	- Working with Kubernetes Objects
	
		- Kubernetes Objects :
		
			- Pods 					# Atomic Unit of Schedule
									# Used to Execute Micro-Services 		
									
			- Deployment Controller Objects :::
			
				- Replicaset				
				- Deployment 
				
			- Kubernetes Services :::
			
				- Cluster-IP Service 
				- NodePort Services 
				- Load Balancer 
				
			- Kubernetes Volumes ::
			
				- EmptyDir 
				- Hostpath Volume 
				- Persistant Volume 
				- Persistant Volume Claim
				
			- Namespace!
			
				
	- Namespace!
	
		- Logical Partitioning of Kubernetes Cluster
		- Namespaces can be created based on Environments, Application Team, Services/Objects 
				
	
		Non-Prod :	-> Dev/QA/UAT 
		
		Kubernetes_Master			(VM)
			Kubernetes_WorkNode1	(VM)
			Kubernetes_WorkNode2	(VM)

	
		Prod :
		
		Kubernetes_Master			(VM)
			Kubernetes_WorkNode1	(VM)
			Kubernetes_WorkNode2	(VM)
			
-  Login to kmaster node
- sudo -i
- kubectl get nodes( You should be able to get list of nodest)
    NAME             STATUS         ROLES        AGE        VERSION
    kmaster-node     Ready         control-plane   46h       v1.29.15
    worker-node1     Ready          <none>         46h       v1.29.15
    worker-node2     Ready          <none>         46h       v1.29.15
- kubectl get pods
- kubectl get pods -- all-namespaces (You will see all the name spaces s in the cluster)
- kubectl get ns 
- kubectl get pods -- all-namespaces -o wide


- In master node kmaster node
- mkdir kubernetes
- cd kubernetes
- ls
- pwd
- /root/kubernetes
- vi nginx-pod.yaml
- copy below from apiVersion to the end past it

*******************************************************************
# 1. 	Create Pod Object Manifest File :

# nginx-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
    tier: dev
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80


*******************************************************************
- cat nginx-pod.yaml

*******************************************************************
2. Create and display Pods

# Create and display PODs

#kubectl apply -f nginx-pod.yaml( To update the manifest file)

kubectl create -f nginx-pod.yaml ( to create the  pod)
kubectl get pod
kubectl get pod -o wide
kubectl get pod nginx-pod -o yaml
kubectl describe pod nginx-pod


*******************************************************************
- kubectl create -f nginx-pod.yaml 
Note: if it gets timed out update the instance type from t2micro to t2.medeium
pod/nginx-pod created
- kubectl get pods
NAME       READY    STATUS     RESTARTS      AGE
nginx-pod    1/1     Running     0            32s
- kubectl get podes -o wide
NAME       READY    STATUS     RESTARTS      AGE        IP         NODE            NOMINATED_NODE READINESS GATES
nginx-pod   1/1     Running     0             72s     10.244.2.2   worker-node2     <none>

- Based upon kubeadm ip range that we have given as reference it's taking fgrom there to provide the ip for POD
- kubectl get pods -o yaml  or kubectl get pods -o json( it will give actual manifest file that store in teh system)
- it's the information from ETCD
- 
3. Test & Delete
# To get inside the pod
kubectl exec -it nginx-pod bash

- ls
bin boot dev docker-entrypoint.d docker-entrypoint.sh etc home lib64 media mnt opt proc root run sbin srv sys tmp usr var
- cd /usr/share/nginx/ (defualt directory for nginx
- ls
- html 
- cd html
ls
50x.html  indexhtml
- root@nginx-pod: /usr/share/nginx/html# 
-

# Create test HTML page
cat <<EOF > /usr/share/nginx/html/test.html
<!DOCTYPE html>
<html>
<head>
<title>Testing..</title>
</head>
<body>
<h1 style="color:rgb(90,70,250);">Hello, Everyone...!</h1>
<h2>Welcome to Kubernetes Demo :-) </h2>
</body>
</html>
EOF
exit

Now in master node
- root @kmaster-node: ~/kubernetes#  kubectl get svc
NAME         TYPE        CLUSTER-IP          EXTERNAL-IP         PORTTS      AGE
kubernetes   clusterIP   10.96.0.1            <none>            443/TCP      47H

# Expose PODS using NodePort service
#NodePort Service Create Port between - 30000 to 32767
# Node-Port Range : 30000 - 32767 

-  - root @kmaster-node: ~/kubernetes#  kubectl expose pod nginx-pod --type=NodePort --port=80

-# Display Service and find NodePort
- root @kmaster-node: ~/kubernetes#   kubectl describe svc nginx-pod

NAME         TYPE        CLUSTER-IP          EXTERNAL-IP         PORTTS      AGE
kubernetes   clusterIP   10.96.0.1            <none>            443/TCP      47H
nginx-pod    NodePort    10.105.105.66        <none>            80:32049/TCP   10s

- # Open Web-browser and access webapge using 
http://<external-nodeip>:<nodeport>/test.html

#eg.: http://43.205.215.212:32049/filename


- root @kmaster-node: ~/kubernetes# kubectl describe svc nginx-pod
Name:          nginx-pod
Namespace      default
Labels:         app=nginx
                tier=dev
Annotations:    <none>
Selector:       app=nginx,tier=dev
Type:           NodePort
IP Family Policy:   SingleStack
IP Families:        IPv4
IP:                 10.105.105.66
IPs:                10.105.105.66
Port:               <unset>    80/TCP
TargetPort:         80/TCP
NodePort:            <unset>    32049/TCP
Endpoints:          10.244.2.2:80
Session Affinity:    None
External Traffic Policy:  Cluster
Events:                   <none>

- root @kmaster-node: ~/kubernetes#  kubectl get pods -o wide

NAME       READY    STATUS     RESTARTS      AGE        IP         NODE            NOMINATED_NODE READINESS GATES
nginx-pod   1/1     Running     0             72s     10.244.2.2   worker-node2     <none>



1:06:29




	- Pods :
	
	
		NodePort Service Create Port between - 30000 to 32767
				
		Roles :
		
			Kubernetes Developers 
			
			Kubernetes Administrators 
			
			Kubernetes Security Administrators 
			
			
			WorkerNode1,2,3,4,5,6,7,8,9,10 
			
				--> 1,3,5,7,9 ==> as Target group 
				
				--> 1 	==> Target Node 

	- Deployment Controller Objects :::
	
		- Replicaset				
		- Deployment 
		
	Controller Object :::
		ReplicaSet 
		Deployment 


	ReplicaSet :::
	
		--> Replicaset is used to execute the specific no. of pods in the cluster.
		--> Replicaset uses the Set Based Operator
		--> Used to replicate the pods and able to scale up/down
		--> The Replicasets will be automatically created, while creating Deployment Controller Object.
	
	Deployment Controller Object :::
	
		--> It is used to deploy the pods and ensure high availability of pods by creating pod replicas 
		--> 1. Create Muliple instance/replicas/copies of pods 
			2. Used to Scale-Up / Scale-Down the Pods 
			3. Used to Upgrade the application pods 
			4. Used to Down-grade/roll-back the application pods
		--> The upgrade/down-grade of application pods can be done without any downtime. 
		--> To achieve zero-downtime during upgrade/down-grade, By Default, it used Rolling-Update Deployment Strategy.
		

	App_snapshot_v1.0		--->	App_snapshot_img:v1.0	==> Published to Dockerhub.
	App_snapshot_v1.1		--->	App_snapshot_img:v1.1	==> Published to Dockerhub.	
	App_snapshot_v1.2		--->	App_snapshot_img:v1.2	==> Published to Dockerhub.	
	
	
	App_snapshot_v1.0		--->	App_snapshot_img:v1.0	==> Published to Dockerhub.
	
	
	Pod1 ==> App_snapshot_img:v1.0	Running!
	
	
	Deploy the Pods using Deployment Controller Object :
	
	
		- Create Deployment Object

		- Replicaset 					Replica = 3.
		
		- Pods Instances 				To Create 3 replicas of pod
		
	
	
	Pod1	=	App_snapshot_img:v1.0
	
	Pod2 	=	App_snapshot_img:v1.0
	
	Pod3 	=	App_snapshot_img:v1.0
	
	
	
	Upgrade Application :		using Rolling-Update Deployment Strategy.
	
	
		App_snapshot_v1.0		--->	App_snapshot_img:v1.1	==> Published to Dockerhub.	
		
		
		
	Pod1	=	App_snapshot_img:v1.0						====>			App_snapshot_img:v1.1	
	
	Pod2 	=	App_snapshot_img:v1.0						====>			App_snapshot_img:v1.1
	
	Pod3 	=	App_snapshot_img:v1.0						====>			App_snapshot_img:v1.1
	
	
	Scale-Up/Scale-Down :::
	
		- Increase/Decrease the Replicas of Pods 



		Web_Application :::
		
			--> Replica = 100 		==> 	1000 Users/Requests 			

				Replica	= 500		===> 	5000 Users/Requests		


			online Reservation System 
			
			amazon.in 
			
			
			Pod Replicas  			>		Taken care by Kubernetes Deployment Controller Object 			
			
			Node Replicas 			>		Add More nodes to the cluster (Horizontal Scaling)
			
			Upgrade the Existing Nodes 	
									> 	Eg.: t2.micro to t2.medium -> Update the Node - Vertical Scaling 			
		
		Kubernetes_Master			(VM)
			Kubernetes_WorkNode1	(VM)
			Kubernetes_WorkNode2	(VM)



*******************************************************************




# Delete pod & svc
kubectl delete svc nginx-pod
kubectl delete pod nginx-pod



#########################
Day 23 : 18th August 2025
#########################

	Working with Deployment Controller Objects!
	
		- Manifest file 
		
			- Defines the properties of Deployment Controller Object!
			
			
			Output :
			
				- Deployment Object 

		     	- ReplicaSet 
				
				- Pod Instances!	

*******************************************************************

- In master kubernetes node
- sudo -i
- kubectl get pods
-cd kubernetes/
- vi nginx-deploy.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-app1
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:stable-otel
        ports:
        - containerPort: 80
  selector:
    matchLabels:
      app: nginx-app

-:wq!

-kubectl create -f nginx-deploy.yaml
deployment.apps/nginx-deploy created
-kubectl get deploy
NAME           READY      UP-TO-DATE       AVAILABLE   AGE
nginx-deploy    3/3                3          1         11s
- kubectl get rs
NAME                       DESIRED       CURRENT   READY     AGE
nginx-deploy-59cfff6f794   3               3        3         41s        11s
-kubectl get pods
NAME                             READY   STATUS      RESTARTS       AGE
nginx-deploy-59cfff6f794-7VL9W    1/1    Running     0               73s
nginx-deploy-59cfff6f794-FZ2D6    1/1    Running     0               73s
nginx-deploy-59cfff6f794-lfpw9    1/1    Running     0               73s

- kubectl get pods -o wide
NAME                             READY   STATUS      RESTARTS       AGE       IP              NODE          NOMINATED NODE   READINESSGATES
nginx-deploy-59cfff6f794-7VL9W    1/1    Running     0               73s      10.244.1.3     worker-node1   <none>             <none>
nginx-deploy-59cfff6f794-FZ2D6    1/1    Running     0               73s      10.244.2.4     worker-node2   <none>             <none>
nginx-deploy-59cfff6f794-lfpw9    1/1    Running     0               73s      10.244.1.2     worker-node1   <none>             <none>


-kubect describe deploy nginx-deploy
- if you delete the pod deploy object will automatically create a new pod
- kubectl delete pod nginx-deploy-59cfff6f794-7VL9W 

- kubectl get pods -o wide
NAME                             READY   STATUS      RESTARTS       AGE       IP              NODE          NOMINATED NODE   READINESSGATES
nginx-deploy-59cfff6f794-7tmhn   1/1    Running     0               73s      10.244.1.5     worker-node2   <none>             <none>
nginx-deploy-59cfff6f794-FZ2D6    1/1    Running     0               73s      10.244.2.4     worker-node2   <none>             <none>
nginx-deploy-59cfff6f794-lfpw9    1/1    Running     0               73s      10.244.1.2     worker-node1   <none>             <none>

- kubectl scale deployment nginx-deploy --replicas=5
- kubectl describe deploy nginx-deploy | grep -i image
    Image:           nginx:stable-otel
- kubectl set image deploy nginx-deploy nginx-container=nginx:stable-perl
- kubectl describe deploy nginx-deploy | grep -i image
    Image:           nginx:stable-perl


-kubectl get deploy
NAME           READY      UP-TO-DATE       AVAILABLE   AGE
nginx-deploy    1/1              1         1           11s
-kubectl get pods
NAME                             READY   STATUS      RESTARTS       AGE
nginx-deploy-59cfff6f794-7VL9W    1/1    Running         0               73s
nginx-deploy-59cfff6f794-FZ2D6    0/1    ErrImagePull    0               73s
-kubectl rollout status deployment/nginx-deploy
failed so ctrl c
-kubectl rollout undo deployment/nginx-deploy
- kubectl describe deploy nginx-deploy | grep -i image
    Image:           nginx:stable-otel

-kubectl get pods
NAME                             READY   STATUS      RESTARTS       AGE
nginx-deploy-59cfff6f794-7VL9W    1/1    Running         0               73s

- kubectl scale deployment nginx-deploy --replicas=3
--kubectl get pods
NAME                             READY   STATUS      RESTARTS       AGE
nginx-deploy-59cfff6f794-7VL9W    1/1    Running     0               73s
nginx-deploy-59cfff6f794-FZ2D6    1/1    Running     0               73s
nginx-deploy-59cfff6f794-lfpw9    1/1    Running     0               73s

- kubectl describe deploy nginx-deploy | grep -i image
    Image:           nginx:stable-otel
-- kubectl describe pod nginx-deploy-59cfff6f794-7VL9W | grep -i image
    Image:           nginx:stable-otel
- kubectl set image deploy nginx-deploy nginx-container=nginx:stable-perl
- kubectl get pods 
NAME                             READY   STATUS      RESTARTS       AGE
nginx-deploy-59cfff6f794-7VL9W    1/1    Running     0               73s
nginx-deploy-59cfff6f794-FZ2D6    1/1    Running     0               73s
nginx-deploy-59cfff6f794-lfpw9    1/1    ImagePullBakoff    0               73s

-kubectl rollout undo deployment/nginx-deploy
- kubectl get deploy nginx-deploy -o yaml (it will query your etcd)



	

	- Kubernetes Services :::
	
		- Cluster-IP Service 			# Default Service!
		- NodePort Services 
         - Load Balancer:

Node Port Service:

https://192.168.1.1:31000
Node Port:[30000 to 32767
                  31000
-----------------192.168.1.1-------------------->--Node IP
|                                               |
|                                               |
|                                               |
|                                               |
|                                               |
|                  80                           |
|          | ---10.180.0.15-----|               |-----> Port
|          |  service: nodePort |               |
|          | ___________________|               |
|                                               |
|                                               |
|                                               |
|                    80   ----------------------|---> Target Port
|           | ----10.2210.0.4-----|             |
|           |     FrontEnd-Pod    |             |
|           | --------------------|             |
------------------------------------------------
            NODE

MultiInstance in same node


                 31000
_______________192.168.1.1__________________________________________________________Node1

________________________________________________
|                    80   ----------------------|--->service Port
|           | ----10.1800.0.4-----|             |
|           |    Selector: app:mypp|            |
|           | --------------------|   
____________________________________________________


------------------------------------------------
|                    80   ----------------------|---> Target Port
|           | ----10.2210.0.4-----|             |
|           |     lables:app-Myapp|             |
|           | --------------------|   
----------------------------------------------------Pod

|                    80   ----------------------|---> Target Port
|           | ----10.2210.0.4-----|             |
|           |   lables:app-Myapp  |             |
|           | --------------------|   
----------------------------------------------------Pod

|                    80   ----------------------|---> Target Port
|           | ----10.2210.0.4-----|             |
|           |    lables:app-Myapp |             |
|           | --------------------|             |
----------------------------------------------------Pod
______________________________________________________________________Node1



                 ---------------http://loksaieta.com
                |
		  - Load Balancer 


   |                                   |
  31000                             31000
-192.168.1.1-                   192.168.1.2           

              10.18.0.15
             slector:
               app: mapp
    ------------------------------------service



     80                                80             
  10.210.0.1                        10.210.0.2
 lablels:                           lablels:
    app: Myapp                        app: Myapp
------------------Node1           ------------------Node2
----------------------------------------------------------------cluster


cluster ip

                 31000
_______________192.168.1.1__________________________________________________________Node1

________________________________________________
|                    80   ----------------------|--->service Port
|           | ----10.1800.0.4-----|             |
|           |    Selector: app:mypp|            |
|           | --------------------|   
____________________________________________________


------------------------------------------------
|                    80   ----------------------|---> Target Port
|           | ----10.2210.0.4-----|             |
|           |     fron end        |             |
|           | --------------------|   
----------------------------------------------------Pod

|                    80   ----------------------|---> Target Port
|           | ----10.2210.0.4-----|             |
|           |   frontend           |             |
|           | --------------------|   
----------------------------------------------------Pod

|                    80   ----------------------|---> Target Port
|           | ----10.2210.0.4-----|             |
|           |    frontend         |             |
|           | --------------------|             |
----------------------------------------------------Pod



________________________________________________
|                                --------|--->clusterIP
|           | ----10.1800.0.4:8080-----|        |
|           |    Selector: app:mypp|            |
|           | --------------------|             |
____________________________________________________



------------------------------------------------
|                    80   ----------------------|---> Target Port
|           | ----10.2210.0.4-----|             |
|           |     backend         |             |
|           | --------------------|   
----------------------------------------------------Pod

|                    80   ----------------------|---> Target Port
|           | ----10.2210.0.4-----|             |
|           |   backend           |             |
|           | --------------------|   
----------------------------------------------------Pod

|                    80   ----------------------|---> Target Port
|           | ----10.2210.0.4-----|             |
|           |    backend          |             |
|           | --------------------|             |
----------------------------------------------------Pod
______________________________________________________________________Node1



		- Ingress Controller!
		
			- Used to reduce the Load Balancer!
		
		
			- Routing!
			
			
		- Simple Routing 				# meant for static web page 
		
		- Host Based Routing 
		
		- Path Routing!
		
		- Query/Parameter Based Routing!
		
		Eg.: 
		
			www.google.com !							Load Balancer URL 
			
			
			www.mail.google.com 						# Host Based Routing!			
			www.maps.google.com 						# Host Based Routing!			
			www.drive.google.com 						# Host Based Routing!			
			www.translate.google.com 					# Host Based Routing!

	
		www.mail.google.com/inbox 						# Path Based Routing!		
		www.mail.google.com/sent 						# Path Based Routing!		
		www.mail.google.com/trash 						# Path Based Routing!		
		www.mail.google.com/compose 					# Path Based Routing!		
		
		www.mail.google.com/inbox 						# Path Based Routing!		
		
		www.mail.google.com/inbox/email=xyz@gmail.com	# Query/Parameter Based Routing







*******************************************************************
*******************************************************************

# 1. Deployment YAML file

# nginx-deploy.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx-app1
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:stable-otel
        ports:
        - containerPort: 80
  selector:
    matchLabels:
      app: nginx-app

*******************************************************************
# 2. Create and Display Deployment

kubectl create -f nginx-deploy.yaml 
kubectl get deploy -l app=nginx-app
kubectl get rs -l app=nginx-app
kubectl get po -l app=nginx-app
kubectl describe deploy nginx-deploy

*******************************************************************
# 3. Testing: Rollback update 

kubectl set image deploy nginx-deploy nginx-container=nginx:stable-pearl
kubectl rollout status deployment/nginx-deploy
kubectl rollout history deployment/nginx-deploy
kubectl rollout undo deployment/nginx-deploy
kubectl rollout status deployment/nginx-deploy
kubectl describe deploy nginx-deploy | grep -i image

*******************************************************************
# 4. Testing: Update Version of "nginx:stable-otel" to "nginx:stable-perl"

kubectl set image deploy nginx-deploy nginx-container=nginx:stable-perl
# or
kubectl edit deploy nginx-deploy
kubectl rollout status deployment/nginx-deploy
kubectl get deploy

*******************************************************************
# 5. Testing: Scale UP

kubectl scale deployment nginx-deploy --replicas=5
kubectl get deploy
kubectl get po -o wide

*******************************************************************
# 6. Testing: Scale DOWN

kubectl scale deployment nginx-deploy --replicas=3
kubectl get deploy
kubectl get po -o wide
   
*******************************************************************

# 7. Cleanup

kubectl delete -f nginx-deploy.yaml
kubectl get deploy
kubectl get rs
kubectl get po 

*******************************************************************


#########################
Day 23 : 19th August 2025
#########################


Purpose of docker container volumes:
---------------------------------------
-Throught out the life cycle of your container, If you want to maintain stateful applicatoins
we will be using container volume, Even if we loose container we should not loose docker volume.
- Similarly in kubernetets we have different volumes


- Kubernetes Volumes:
        - Empty Dir  

        - Hostpath Volume # can be compared with docker volume     
                           (as like we created a volume in the host machine and that will be referereed in the containers, Here in POD)
        - Persistant Volume
        - Persistant Volume Claim

Containerlevel and Pod Level storage:
- TO manage a huge transaction data we cannot fulfill with host path volume. We need to procure the volume from external sources
  like efs ebs persistant dIsk from azure.
- if ant external storage server if we need to access we will use help of persistant volume or persistant volume claim
- Users will not have acess to create persistant volume . Storage and kubernetes admain creates it.
- Devops will not have acess to create persistant volume.
- So, if we compare to continaer volue. In container we can mount one volume, if i have one more container,  same volume can be mounted to another container
- if it is kubenetes, we will not deploy any container we will deploy pod. so that pod will be mountedd with the volume
- Once the pod is allocated to the volume, They container uses the volume  and another container can also uses the volume
- Hece volume can be shared.


- Namespace!
      -Logical Partitioning of Kubernetes cluster
      -Namespaces can be created based on Environments, Appliation team,Services/objects
      - working with namespace


- Integrate Kubernetes with Namespace

    - CICD 




GCEPersistent Disk
AWSElasticBlockStore
AzureDisk
Clinder(OpenStack Block Storage)
FC( Fibre Channel)
RBD(Ceph Block Device)
Flocker 

NFS
ISCSI
CEPHFS
Glusterfs
AzureFile
FlexVolume

Vsphere Volume
Quobyte Volumes
Hostpath
Portworx Volumes
ScaleIO volumes
StorageOS

ExternalStorage:
- In general, Kubernetes support many volumes 
- The storage service.server can be procured from external medium
- In real time envirionment, for any partiuclar transations, we need to maintain persistent dtata
- During autoscaling ,Imagine if i have 5 worker nodes, I will be deploying pods in all workes nodes, if i reduce the count to 2,
  the worker nodes can be deleted, so we cannot maintain volume from node level.
- Hence we rely on external storage
- In general storage servers categorized as a block storage
Block storage, NFS, Ojbect sotrage , others

- How this is procured, Lets say im a dev, ihave prepared application artificats,
  i want to execute as a pod. so execute this pod, i need around 1 TB of volume
-  We have concept called Persistan Volume


Persistent Volumes and Persisten Volume claim:
- Abstracts details of hos storage is provided from how it is consumerd
- Persistentt volume PV (Piece of storage in cluster) and Persistent volume clainm(PVC) ( Request for Storage)
- It's responsible of storage administrators to create some storage in cluster for pod deployemnet
- Once the volume is available in persistant volume pool
- if you need 2 gb for pod deployment, you can create a request for storage is called as perisistenet volume claim
- Now how this volume is allocated to the od
- Two typoes of provisions
- Static Provision and Dynamic Provision:
  -when ever you are sure about the actual storage, then we can go with static provision. vOOLUMEN CAN BE 
   procured from any external storage server like SSD  HDD NFS object 
  - storage admin will created the persistant volumes in persistant volume pool
  - Once they user the request for how much is needed and what type of volume needed, it will created by admins
  - if the volume is available in the pools can be reserved for pod deployment.
  - if requested volume is not avialalbe, then it will use the storage class parameter  based up allocation dynamically


1.HostPath YAML file

apiVersion: V1
kind: Pod
metadata: 
  name: nginx-hostpath
spec:
  containers:
    - name: nginx-containers
      image: nginx
      volumeMounts:
      - mountPath: /test-mnt
        name: test-vol
  volumes:
  - name: test vol
    hostPath:
      path: /test-vol

        --mount source=vol1,destination=/vol1

This is Static Provisioning:
-----------------------------
Persistent Volume(PV)
---------------------------------

#pv.yaml;
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-gce
spec:
  capacity:
    storage: 15Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: slow
  gcePersistentDisk: 
    pdName: my-disk-123
    fsType: ext4

Persistent Volume Clain(PVC)
---------------------------------

#pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-disk-claim
spec:
  resources:
    requests:
      storage: 15Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: slow


Referencing Claim in Pod
-------------------------------
# nginx-pv.yaml

apiVersion: v1
kind: Pod
metadata: 
  name: pv-pod
spec:
  containers:
  - name: test-container
    image: nginx
    volumeMounts:
    - mountPath: /test-d
      name: test-volume
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName:my-disk-claim



Dynamic Provisioning:
----------------------


Storage Class
-----------
kind: StorageClass
apiVersion: Storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd


Persistent Volume Claim (PVC)
----------------------------------
#pvc-dv1.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata: 
  name: my-disk-claim-1
spec:
  resources:
    requests:    
      storage: 30Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: fast


Referencing claim in Pod
----------------------------

# nginx-pv.yaml
apiVersion:v1
kind: Pod
metadata:
  name: pv-pod
spec:
  containers:
  - name: test-container
    image: nginx
    volumeMounts: 
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: my-disk-claim-1














